{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module tensorflow.python.training.training in tensorflow.python.training:\n",
      "\n",
      "NAME\n",
      "    tensorflow.python.training.training - This library provides a set of classes and functions that helps train models.\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training.py\n",
      "\n",
      "DESCRIPTION\n",
      "    ## Optimizers\n",
      "    \n",
      "    The Optimizer base class provides methods to compute gradients for a loss and\n",
      "    apply gradients to variables.  A collection of subclasses implement classic\n",
      "    optimization algorithms such as GradientDescent and Adagrad.\n",
      "    \n",
      "    You never instantiate the Optimizer class itself, but instead instantiate one\n",
      "    of the subclasses.\n",
      "    \n",
      "    @@Optimizer\n",
      "    \n",
      "    @@GradientDescentOptimizer\n",
      "    @@AdadeltaOptimizer\n",
      "    @@AdagradOptimizer\n",
      "    @@MomentumOptimizer\n",
      "    @@AdamOptimizer\n",
      "    @@FtrlOptimizer\n",
      "    @@RMSPropOptimizer\n",
      "    \n",
      "    ## Gradient Computation\n",
      "    \n",
      "    TensorFlow provides functions to compute the derivatives for a given\n",
      "    TensorFlow computation graph, adding operations to the graph. The\n",
      "    optimizer classes automatically compute derivatives on your graph, but\n",
      "    creators of new Optimizers or expert users can call the lower-level\n",
      "    functions below.\n",
      "    \n",
      "    @@gradients\n",
      "    @@AggregationMethod\n",
      "    \n",
      "    @@stop_gradient\n",
      "    \n",
      "    \n",
      "    ## Gradient Clipping\n",
      "    \n",
      "    TensorFlow provides several operations that you can use to add clipping\n",
      "    functions to your graph. You can use these functions to perform general data\n",
      "    clipping, but they're particularly useful for handling exploding or vanishing\n",
      "    gradients.\n",
      "    \n",
      "    @@clip_by_value\n",
      "    @@clip_by_norm\n",
      "    @@clip_by_average_norm\n",
      "    @@clip_by_global_norm\n",
      "    @@global_norm\n",
      "    \n",
      "    ## Decaying the learning rate\n",
      "    @@exponential_decay\n",
      "    \n",
      "    ## Moving Averages\n",
      "    \n",
      "    Some training algorithms, such as GradientDescent and Momentum often benefit\n",
      "    from maintaining a moving average of variables during optimization.  Using the\n",
      "    moving averages for evaluations often improve results significantly.\n",
      "    \n",
      "    @@ExponentialMovingAverage\n",
      "    \n",
      "    ## Coordinator and QueueRunner\n",
      "    \n",
      "    See [Threading and Queues](../../how_tos/threading_and_queues/index.md)\n",
      "    for how to use threads and queues.  For documentation on the Queue API,\n",
      "    see [Queues](../../api_docs/python/io_ops.md#queues).\n",
      "    \n",
      "    @@Coordinator\n",
      "    @@QueueRunner\n",
      "    @@add_queue_runner\n",
      "    @@start_queue_runners\n",
      "    \n",
      "    ## Distributed execution\n",
      "    \n",
      "    See [Distributed TensorFlow](../../how_tos/distributed/index.md) for\n",
      "    more information about how to configure a distributed TensorFlow program.\n",
      "    \n",
      "    @@Server\n",
      "    @@Supervisor\n",
      "    @@SessionManager\n",
      "    @@ClusterSpec\n",
      "    @@replica_device_setter\n",
      "    \n",
      "    ## Summary Operations\n",
      "    \n",
      "    The following ops output\n",
      "    [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\n",
      "    protocol buffers as serialized string tensors.\n",
      "    \n",
      "    You can fetch the output of a summary op in a session, and pass it to\n",
      "    a [SummaryWriter](../../api_docs/python/train.md#SummaryWriter) to append it\n",
      "    to an event file.  Event files contain\n",
      "    [`Event`](https://www.tensorflow.org/code/tensorflow/core/util/event.proto)\n",
      "    protos that can contain `Summary` protos along with the timestamp and\n",
      "    step.  You can then use TensorBoard to visualize the contents of the\n",
      "    event files.  See [TensorBoard and\n",
      "    Summaries](../../how_tos/summaries_and_tensorboard/index.md) for more\n",
      "    details.\n",
      "    \n",
      "    @@scalar_summary\n",
      "    @@image_summary\n",
      "    @@audio_summary\n",
      "    @@histogram_summary\n",
      "    @@zero_fraction\n",
      "    \n",
      "    @@merge_summary\n",
      "    @@merge_all_summaries\n",
      "    \n",
      "    ## Adding Summaries to Event Files\n",
      "    \n",
      "    See [Summaries and\n",
      "    TensorBoard](../../how_tos/summaries_and_tensorboard/index.md) for an\n",
      "    overview of summaries, event files, and visualization in TensorBoard.\n",
      "    \n",
      "    @@SummaryWriter\n",
      "    @@summary_iterator\n",
      "    \n",
      "    ## Training utilities\n",
      "    \n",
      "    @@global_step\n",
      "    @@write_graph\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        tensorflow.python.training.coordinator.Coordinator\n",
      "        tensorflow.python.training.moving_averages.ExponentialMovingAverage\n",
      "        tensorflow.python.training.optimizer.Optimizer\n",
      "            tensorflow.python.training.adadelta.AdadeltaOptimizer\n",
      "            tensorflow.python.training.adagrad.AdagradOptimizer\n",
      "            tensorflow.python.training.adam.AdamOptimizer\n",
      "            tensorflow.python.training.ftrl.FtrlOptimizer\n",
      "            tensorflow.python.training.gradient_descent.GradientDescentOptimizer\n",
      "            tensorflow.python.training.momentum.MomentumOptimizer\n",
      "            tensorflow.python.training.rmsprop.RMSPropOptimizer\n",
      "        tensorflow.python.training.queue_runner.QueueRunner\n",
      "        tensorflow.python.training.saver.Saver\n",
      "        tensorflow.python.training.server_lib.ClusterSpec\n",
      "        tensorflow.python.training.server_lib.Server\n",
      "        tensorflow.python.training.session_manager.SessionManager\n",
      "        tensorflow.python.training.summary_io.SummaryWriter\n",
      "        tensorflow.python.training.supervisor.Supervisor\n",
      "    google.protobuf.message.Message(__builtin__.object)\n",
      "        tensorflow.core.example.example_pb2.Example\n",
      "        tensorflow.core.example.example_pb2.SequenceExample\n",
      "        tensorflow.core.example.feature_pb2.BytesList\n",
      "        tensorflow.core.example.feature_pb2.Feature\n",
      "        tensorflow.core.example.feature_pb2.FeatureList\n",
      "        tensorflow.core.example.feature_pb2.FeatureLists\n",
      "        tensorflow.core.example.feature_pb2.Features\n",
      "        tensorflow.core.example.feature_pb2.FloatList\n",
      "        tensorflow.core.example.feature_pb2.Int64List\n",
      "        tensorflow.core.protobuf.saver_pb2.SaverDef\n",
      "    threading.Thread(threading._Verbose)\n",
      "        tensorflow.python.training.coordinator.LooperThread\n",
      "    \n",
      "    class AdadeltaOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the Adadelta algorithm. \n",
      "     |  \n",
      "     |  See [M. D. Zeiler](http://arxiv.org/abs/1212.5701)\n",
      "     |  ([pdf](http://arxiv.org/pdf/1212.5701v1.pdf))\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdadeltaOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name='Adadelta')\n",
      "     |      Construct a new Adadelta optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A `Tensor` or a floating point value. The learning rate.\n",
      "     |        rho: A `Tensor` or a floating point value. The decay rate.\n",
      "     |        epsilon: A `Tensor` or a floating point value.  A constant epsilon used\n",
      "     |                 to better conditioning the grad update.\n",
      "     |        use_locking: If `True` use locks for update operations.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients.  Defaults to \"Adadelta\".\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class AdagradOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the Adagrad algorithm.\n",
      "     |  \n",
      "     |  See this [paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdagradOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate, initial_accumulator_value=0.1, use_locking=False, name='Adagrad')\n",
      "     |      Construct a new Adagrad optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A `Tensor` or a floating point value.  The learning rate.\n",
      "     |        initial_accumulator_value: A floating point value.\n",
      "     |          Starting value for the accumulators, must be positive.\n",
      "     |        use_locking: If `True` use locks for update operations.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients.  Defaults to \"Adagrad\".\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the `initial_accumulator_value` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class AdamOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the Adam algorithm.\n",
      "     |  \n",
      "     |  See [Kingma et. al., 2014](http://arxiv.org/abs/1412.6980)\n",
      "     |  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdamOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      "     |      Construct a new Adam optimizer.\n",
      "     |      \n",
      "     |      Initialization:\n",
      "     |      \n",
      "     |      ```\n",
      "     |      m_0 <- 0 (Initialize initial 1st moment vector)\n",
      "     |      v_0 <- 0 (Initialize initial 2nd moment vector)\n",
      "     |      t <- 0 (Initialize timestep)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      The update rule for `variable` with gradient `g` uses an optimization\n",
      "     |      described at the end of section2 of the paper:\n",
      "     |      \n",
      "     |      ```\n",
      "     |      t <- t + 1\n",
      "     |      lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
      "     |      \n",
      "     |      m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
      "     |      v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
      "     |      variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      The default value of 1e-8 for epsilon might not be a good default in\n",
      "     |      general. For example, when training an Inception network on ImageNet a\n",
      "     |      current good choice is 1.0 or 0.1.\n",
      "     |      \n",
      "     |      Note that in dense implement of this algorithm, m_t, v_t and variable will \n",
      "     |      update even if g is zero, but in sparse implement, m_t, v_t and variable \n",
      "     |      will not update in iterations g is zero.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      "     |        beta1: A float value or a constant float tensor.\n",
      "     |          The exponential decay rate for the 1st moment estimates.\n",
      "     |        beta2: A float value or a constant float tensor.\n",
      "     |          The exponential decay rate for the 2nd moment estimates.\n",
      "     |        epsilon: A small constant for numerical stability.\n",
      "     |        use_locking: If True use locks for update operations.\n",
      "     |        name: Optional name for the operations created when applying gradients.\n",
      "     |          Defaults to \"Adam\".\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class BytesList(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      BytesList\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  value\n",
      "     |      Magic attribute generated for \"value\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  VALUE_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeRepeatedField>, None)}\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class ClusterSpec(__builtin__.object)\n",
      "     |  Represents a cluster as a set of \"tasks\", organized into \"jobs\".\n",
      "     |  \n",
      "     |  A `tf.train.ClusterSpec` represents the set of processes that\n",
      "     |  participate in a distributed TensorFlow computation. Every\n",
      "     |  [`tf.train.Server`](#Server) is constructed in a particular cluster.\n",
      "     |  \n",
      "     |  To create a cluster with two jobs and five tasks, you specify the\n",
      "     |  mapping from job names to lists of network addresses (typically\n",
      "     |  hostname-port pairs).\n",
      "     |  \n",
      "     |  ```\n",
      "     |  cluster = tf.train.ClusterSpec({\"worker\": [\"worker0.example.com:2222\",\n",
      "     |                                             \"worker1.example.com:2222\",\n",
      "     |                                             \"worker2.example.com:2222\"],\n",
      "     |                                  \"ps\": [\"ps0.example.com:2222\",\n",
      "     |                                         \"ps1.example.com:2222\"]})\n",
      "     |  ```\n",
      "     |  \n",
      "     |  @@as_cluster_def\n",
      "     |  @@as_dict\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, cluster)\n",
      "     |      Creates a `ClusterSpec`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        cluster: A dictionary mapping one or more job names to lists of network\n",
      "     |          addresses, or a `tf.train.ClusterDef` protocol buffer.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `cluster` is not a dictionary mapping strings to lists\n",
      "     |          of strings, and not a `tf.train.ClusterDef` protobuf.\n",
      "     |  \n",
      "     |  as_cluster_def(self)\n",
      "     |      Returns a `tf.train.ClusterDef` protocol buffer based on this cluster.\n",
      "     |  \n",
      "     |  as_dict(self)\n",
      "     |      Returns a dictionary from job names to lists of network addresses.\n",
      "     |  \n",
      "     |  job_tasks(self, job_name)\n",
      "     |      Returns a list of tasks in the given job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        job_name: The string name of a job in this cluster.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings, corresponding to the network addresses of tasks in\n",
      "     |        the given job, ordered by task index.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If `job_name` does not name a job in this cluster.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  jobs\n",
      "     |      Returns a list of job names in this cluster.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings, corresponding to the names of jobs in this cluster.\n",
      "    \n",
      "    class Coordinator(__builtin__.object)\n",
      "     |  A coordinator for threads.\n",
      "     |  \n",
      "     |  This class implements a simple mechanism to coordinate the termination of a\n",
      "     |  set of threads.\n",
      "     |  \n",
      "     |  #### Usage:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Create a coordinator.\n",
      "     |  coord = Coordinator()\n",
      "     |  # Start a number of threads, passing the coordinator to each of them.\n",
      "     |  ...start thread 1...(coord, ...)\n",
      "     |  ...start thread N...(coord, ...)\n",
      "     |  # Wait for all the threads to terminate.\n",
      "     |  coord.join(threads)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Any of the threads can call `coord.request_stop()` to ask for all the threads\n",
      "     |  to stop.  To cooperate with the requests, each thread must check for\n",
      "     |  `coord.should_stop()` on a regular basis.  `coord.should_stop()` returns\n",
      "     |  `True` as soon as `coord.request_stop()` has been called.\n",
      "     |  \n",
      "     |  A typical thread running with a coordinator will do something like:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  while not coord.should_stop():\n",
      "     |    ...do some work...\n",
      "     |  ```\n",
      "     |  \n",
      "     |  #### Exception handling:\n",
      "     |  \n",
      "     |  A thread can report an exception to the coordinator as part of the\n",
      "     |  `should_stop()` call.  The exception will be re-raised from the\n",
      "     |  `coord.join()` call.\n",
      "     |  \n",
      "     |  Thread code:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  try:\n",
      "     |    while not coord.should_stop():\n",
      "     |      ...do some work...\n",
      "     |  except Exception as e:\n",
      "     |    coord.request_stop(e)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Main code:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  try:\n",
      "     |    ...\n",
      "     |    coord = Coordinator()\n",
      "     |    # Start a number of threads, passing the coordinator to each of them.\n",
      "     |    ...start thread 1...(coord, ...)\n",
      "     |    ...start thread N...(coord, ...)\n",
      "     |    # Wait for all the threads to terminate.\n",
      "     |    coord.join(threads)\n",
      "     |  except Exception as e:\n",
      "     |    ...exception that was passed to coord.request_stop()\n",
      "     |  ```\n",
      "     |  \n",
      "     |  To simplify the thread implementation, the Coordinator provides a\n",
      "     |  context handler `stop_on_exception()` that automatically requests a stop if\n",
      "     |  an exception is raised.  Using the context handler the thread code above\n",
      "     |  can be written as:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  with coord.stop_on_exception():\n",
      "     |    while not coord.should_stop():\n",
      "     |      ...do some work...\n",
      "     |  ```\n",
      "     |  \n",
      "     |  #### Grace period for stopping:\n",
      "     |  \n",
      "     |  After a thread has called `coord.request_stop()` the other threads have a\n",
      "     |  fixed time to stop, this is called the 'stop grace period' and defaults to 2\n",
      "     |  minutes.  If any of the threads is still alive after the grace period expires\n",
      "     |  `coord.join()` raises a RuntimeException reporting the laggards.\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  try:\n",
      "     |    ...\n",
      "     |    coord = Coordinator()\n",
      "     |    # Start a number of threads, passing the coordinator to each of them.\n",
      "     |    ...start thread 1...(coord, ...)\n",
      "     |    ...start thread N...(coord, ...)\n",
      "     |    # Wait for all the threads to terminate, give them 10s grace period\n",
      "     |    coord.join(threads, stop_grace_period_secs=10)\n",
      "     |  except RuntimeException:\n",
      "     |    ...one of the threads took more than 10s to stop after request_stop()\n",
      "     |    ...was called.\n",
      "     |  except Exception:\n",
      "     |    ...exception that was passed to coord.request_stop()\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, clean_stop_exception_types=None)\n",
      "     |      Create a new Coordinator.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        clean_stop_exception_types: Optional tuple of Exception types that should\n",
      "     |          cause a clean stop of the coordinator. If an exception of one of these\n",
      "     |          types is reported to `request_stop(ex)` the coordinator will behave as\n",
      "     |          if `request_stop(None)` was called.  Defaults to\n",
      "     |          `(tf.errors.OutOfRangeError,)` which is used by input queues to signal\n",
      "     |          the end of input. When feeding training data from a Python iterator it\n",
      "     |          is common to add `StopIteration` to this list.\n",
      "     |  \n",
      "     |  clear_stop(self)\n",
      "     |      Clears the stop flag.\n",
      "     |      \n",
      "     |      After this is called, calls to `should_stop()` will return `False`.\n",
      "     |  \n",
      "     |  join(self, threads=None, stop_grace_period_secs=120)\n",
      "     |      Wait for threads to terminate.\n",
      "     |      \n",
      "     |      This call blocks until a set of threads have terminated.  The set of thread\n",
      "     |      is the union of the threads passed in the `threads` argument and the list\n",
      "     |      of threads that registered with the coordinator by calling\n",
      "     |      `Coordinator.register_thread()`.\n",
      "     |      \n",
      "     |      After the threads stop, if an `exc_info` was passed to `request_stop`, that\n",
      "     |      exception is re-raised.\n",
      "     |      \n",
      "     |      Grace period handling: When `request_stop()` is called, threads are given\n",
      "     |      'stop_grace_period_secs' seconds to terminate.  If any of them is still\n",
      "     |      alive after that period expires, a `RuntimeError` is raised.  Note that if\n",
      "     |      an `exc_info` was passed to `request_stop()` then it is raised instead of\n",
      "     |      that `RuntimeError`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        threads: List of `threading.Threads`. The started threads to join in\n",
      "     |          addition to the registered threads.\n",
      "     |        stop_grace_period_secs: Number of seconds given to threads to stop after\n",
      "     |          `request_stop()` has been called.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        RuntimeError: If any thread is still alive after `request_stop()`\n",
      "     |          is called and the grace period expires.\n",
      "     |  \n",
      "     |  register_thread(self, thread)\n",
      "     |      Register a thread to join.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        thread: A Python thread to join.\n",
      "     |  \n",
      "     |  request_stop(self, ex=None)\n",
      "     |      Request that the threads stop.\n",
      "     |      \n",
      "     |      After this is called, calls to `should_stop()` will return `True`.\n",
      "     |      \n",
      "     |      Note: If an exception is being passed in, in must be in the context of\n",
      "     |      handling the exception (i.e. `try: ... except Exception as ex: ...`) and not\n",
      "     |      a newly created one.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        ex: Optional `Exception`, or Python `exc_info` tuple as returned by\n",
      "     |          `sys.exc_info()`.  If this is the first call to `request_stop()` the\n",
      "     |          corresponding exception is recorded and re-raised from `join()`.\n",
      "     |  \n",
      "     |  should_stop(self)\n",
      "     |      Check if stop was requested.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True if a stop was requested.\n",
      "     |  \n",
      "     |  stop_on_exception(*args, **kwds)\n",
      "     |      Context manager to request stop when an Exception is raised.\n",
      "     |      \n",
      "     |      Code that uses a coordinator must catch exceptions and pass\n",
      "     |      them to the `request_stop()` method to stop the other threads\n",
      "     |      managed by the coordinator.\n",
      "     |      \n",
      "     |      This context handler simplifies the exception handling.\n",
      "     |      Use it as follows:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      with coord.stop_on_exception():\n",
      "     |        # Any exception raised in the body of the with\n",
      "     |        # clause is reported to the coordinator before terminating\n",
      "     |        # the execution of the body.\n",
      "     |        ...body...\n",
      "     |      ```\n",
      "     |      \n",
      "     |      This is completely equivalent to the slightly longer code:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      try:\n",
      "     |        ...body...\n",
      "     |      exception Exception as ex:\n",
      "     |        coord.request_stop(ex)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |        nothing.\n",
      "     |  \n",
      "     |  wait_for_stop(self, timeout=None)\n",
      "     |      Wait till the Coordinator is told to stop.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        timeout: Float.  Sleep for up to that many seconds waiting for\n",
      "     |          should_stop() to become True.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True if the Coordinator is told stop, False if the timeout expired.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  joined\n",
      "    \n",
      "    class Example(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      Example\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  features\n",
      "     |      Magic attribute generated for \"features\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FEATURES_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeField>, None)}\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class ExponentialMovingAverage(__builtin__.object)\n",
      "     |  Maintains moving averages of variables by employing an exponential decay.\n",
      "     |  \n",
      "     |  When training a model, it is often beneficial to maintain moving averages of\n",
      "     |  the trained parameters.  Evaluations that use averaged parameters sometimes\n",
      "     |  produce significantly better results than the final trained values.\n",
      "     |  \n",
      "     |  The `apply()` method adds shadow copies of trained variables and add ops that\n",
      "     |  maintain a moving average of the trained variables in their shadow copies.\n",
      "     |  It is used when building the training model.  The ops that maintain moving\n",
      "     |  averages are typically run after each training step.\n",
      "     |  The `average()` and `average_name()` methods give access to the shadow\n",
      "     |  variables and their names.  They are useful when building an evaluation\n",
      "     |  model, or when restoring a model from a checkpoint file.  They help use the\n",
      "     |  moving averages in place of the last trained values for evaluations.\n",
      "     |  \n",
      "     |  The moving averages are computed using exponential decay.  You specify the\n",
      "     |  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n",
      "     |  variables are initialized with the same initial values as the trained\n",
      "     |  variables.  When you run the ops to maintain the moving averages, each\n",
      "     |  shadow variable is updated with the formula:\n",
      "     |  \n",
      "     |    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n",
      "     |  \n",
      "     |  This is mathematically equivalent to the classic formula below, but the use\n",
      "     |  of an `assign_sub` op (the `\"-=\"` in the formula) allows concurrent lockless\n",
      "     |  updates to the variables:\n",
      "     |  \n",
      "     |    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n",
      "     |  \n",
      "     |  Reasonable values for `decay` are close to 1.0, typically in the\n",
      "     |  multiple-nines range: 0.999, 0.9999, etc.\n",
      "     |  \n",
      "     |  Example usage when creating a training model:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Create variables.\n",
      "     |  var0 = tf.Variable(...)\n",
      "     |  var1 = tf.Variable(...)\n",
      "     |  # ... use the variables to build a training model...\n",
      "     |  ...\n",
      "     |  # Create an op that applies the optimizer.  This is what we usually\n",
      "     |  # would use as a training op.\n",
      "     |  opt_op = opt.minimize(my_loss, [var0, var1])\n",
      "     |  \n",
      "     |  # Create an ExponentialMovingAverage object\n",
      "     |  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
      "     |  \n",
      "     |  # Create the shadow variables, and add ops to maintain moving averages\n",
      "     |  # of var0 and var1.\n",
      "     |  maintain_averages_op = ema.apply([var0, var1])\n",
      "     |  \n",
      "     |  # Create an op that will update the moving averages after each training\n",
      "     |  # step.  This is what we will use in place of the usual training op.\n",
      "     |  with tf.control_dependencies([opt_op]):\n",
      "     |      training_op = tf.group(maintain_averages_op)\n",
      "     |  \n",
      "     |  ...train the model by running training_op...\n",
      "     |  ```\n",
      "     |  \n",
      "     |  There are two ways to use the moving averages for evaluations:\n",
      "     |  \n",
      "     |  *  Build a model that uses the shadow variables instead of the variables.\n",
      "     |     For this, use the `average()` method which returns the shadow variable\n",
      "     |     for a given variable.\n",
      "     |  *  Build a model normally but load the checkpoint files to evaluate by using\n",
      "     |     the shadow variable names.  For this use the `average_name()` method.  See\n",
      "     |     the [Saver class](../../api_docs/python/train.md#Saver) for more\n",
      "     |     information on restoring saved variables.\n",
      "     |  \n",
      "     |  Example of restoring the shadow variable values:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Create a Saver that loads variables from their saved shadow values.\n",
      "     |  shadow_var0_name = ema.average_name(var0)\n",
      "     |  shadow_var1_name = ema.average_name(var1)\n",
      "     |  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n",
      "     |  saver.restore(...checkpoint filename...)\n",
      "     |  # var0 and var1 now hold the moving average values\n",
      "     |  ```\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  @@apply\n",
      "     |  @@average_name\n",
      "     |  @@average\n",
      "     |  @@variables_to_restore\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, decay, num_updates=None, name='ExponentialMovingAverage')\n",
      "     |      Creates a new ExponentialMovingAverage object.\n",
      "     |      \n",
      "     |      The `apply()` method has to be called to create shadow variables and add\n",
      "     |      ops to maintain moving averages.\n",
      "     |      \n",
      "     |      The optional `num_updates` parameter allows one to tweak the decay rate\n",
      "     |      dynamically. .  It is typical to pass the count of training steps, usually\n",
      "     |      kept in a variable that is incremented at each step, in which case the\n",
      "     |      decay rate is lower at the start of training.  This makes moving averages\n",
      "     |      move faster.  If passed, the actual decay rate used is:\n",
      "     |      \n",
      "     |        `min(decay, (1 + num_updates) / (10 + num_updates))`\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        decay: Float.  The decay to use.\n",
      "     |        num_updates: Optional count of number of updates applied to variables.\n",
      "     |        name: String. Optional prefix name to use for the name of ops added in\n",
      "     |          `apply()`.\n",
      "     |  \n",
      "     |  apply(self, var_list=None)\n",
      "     |      Maintains moving averages of variables.\n",
      "     |      \n",
      "     |      `var_list` must be a list of `Variable` or `Tensor` objects.  This method\n",
      "     |      creates shadow variables for all elements of `var_list`.  Shadow variables\n",
      "     |      for `Variable` objects are initialized to the variable's initial value.\n",
      "     |      They will be added to the `GraphKeys.MOVING_AVERAGE_VARIABLES` collection.\n",
      "     |      For `Tensor` objects, the shadow variables are initialized to 0.\n",
      "     |      \n",
      "     |      shadow variables are created with `trainable=False` and added to the\n",
      "     |      `GraphKeys.ALL_VARIABLES` collection.  They will be returned by calls to\n",
      "     |      `tf.all_variables()`.\n",
      "     |      \n",
      "     |      Returns an op that updates all shadow variables as described above.\n",
      "     |      \n",
      "     |      Note that `apply()` can be called multiple times with different lists of\n",
      "     |      variables.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var_list: A list of Variable or Tensor objects. The variables\n",
      "     |          and Tensors must be of types float16, float32, or float64.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the moving averages.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If the arguments are not all float16, float32, or float64.\n",
      "     |        ValueError: If the moving average of one of the variables is already\n",
      "     |          being computed.\n",
      "     |  \n",
      "     |  average(self, var)\n",
      "     |      Returns the `Variable` holding the average of `var`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A `Variable` object.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `Variable` object or `None` if the moving average of `var`\n",
      "     |        is not maintained..\n",
      "     |  \n",
      "     |  average_name(self, var)\n",
      "     |      Returns the name of the `Variable` holding the average for `var`.\n",
      "     |      \n",
      "     |      The typical scenario for `ExponentialMovingAverage` is to compute moving\n",
      "     |      averages of variables during training, and restore the variables from the\n",
      "     |      computed moving averages during evaluations.\n",
      "     |      \n",
      "     |      To restore variables, you have to know the name of the shadow variables.\n",
      "     |      That name and the original variable can then be passed to a `Saver()` object\n",
      "     |      to restore the variable from the moving average value with:\n",
      "     |        `saver = tf.train.Saver({ema.average_name(var): var})`\n",
      "     |      \n",
      "     |      `average_name()` can be called whether or not `apply()` has been called.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A `Variable` object.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A string: The name of the variable that will be used or was used\n",
      "     |        by the `ExponentialMovingAverage class` to hold the moving average of\n",
      "     |        `var`.\n",
      "     |  \n",
      "     |  variables_to_restore(self, moving_avg_variables=None)\n",
      "     |      Returns a map of names to `Variables` to restore.\n",
      "     |      \n",
      "     |      If a variable has a moving average, use the moving average variable name as\n",
      "     |      the restore name; otherwise, use the variable name.\n",
      "     |      \n",
      "     |      For example,\n",
      "     |      \n",
      "     |      ```python\n",
      "     |        variables_to_restore = ema.variables_to_restore()\n",
      "     |        saver = tf.train.Saver(variables_to_restore)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Below is an example of such mapping:\n",
      "     |      \n",
      "     |      ```\n",
      "     |        conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,\n",
      "     |        conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,\n",
      "     |        global_step: global_step\n",
      "     |      ```\n",
      "     |      Args:\n",
      "     |        moving_avg_variables: a list of variables that require to use of the\n",
      "     |          moving variable name to be restored. If None, it will default to\n",
      "     |          variables.moving_average_variables() + variables.trainable_variables()\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A map from restore_names to variables. The restore_name can be the\n",
      "     |        moving_average version of the variable name if it exist, or the original\n",
      "     |        variable name.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Feature(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      Feature\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  bytes_list\n",
      "     |      Magic attribute generated for \"bytes_list\" proto field.\n",
      "     |  \n",
      "     |  float_list\n",
      "     |      Magic attribute generated for \"float_list\" proto field.\n",
      "     |  \n",
      "     |  int64_list\n",
      "     |      Magic attribute generated for \"int64_list\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  BYTES_LIST_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FLOAT_LIST_FIELD_NUMBER = 2\n",
      "     |  \n",
      "     |  INT64_LIST_FIELD_NUMBER = 3\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeField>, <google.protobuf.de...\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class FeatureList(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      FeatureList\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  feature\n",
      "     |      Magic attribute generated for \"feature\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FEATURE_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeRepeatedField>, None)}\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class FeatureLists(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      FeatureLists\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  feature_list\n",
      "     |      Magic attribute generated for \"feature_list\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FEATURE_LIST_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  FeatureListEntry = <class 'tensorflow.core.example.feature_pb2.Feature...\n",
      "     |  \n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeMap>, None)}\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class Features(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      Features\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  feature\n",
      "     |      Magic attribute generated for \"feature\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FEATURE_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  FeatureEntry = <class 'tensorflow.core.example.feature_pb2.FeatureEntr...\n",
      "     |  \n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeMap>, None)}\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class FloatList(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      FloatList\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  value\n",
      "     |      Magic attribute generated for \"value\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  VALUE_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodePackedField>, None), '\\r': ...\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class FtrlOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the FTRL algorithm.\n",
      "     |  \n",
      "     |  See this [paper](\n",
      "     |  https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf).\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FtrlOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=0.0, l2_regularization_strength=0.0, use_locking=False, name='Ftrl')\n",
      "     |      Construct a new FTRL optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A float value or a constant float `Tensor`.\n",
      "     |        learning_rate_power: A float value, must be less or equal to zero.\n",
      "     |        initial_accumulator_value: The starting value for accumulators.\n",
      "     |          Only positive values are allowed.\n",
      "     |        l1_regularization_strength: A float value, must be greater than or\n",
      "     |          equal to zero.\n",
      "     |        l2_regularization_strength: A float value, must be greater than or\n",
      "     |          equal to zero.\n",
      "     |        use_locking: If `True` use locks for update operations.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients.  Defaults to \"Ftrl\".\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If one of the arguments is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class GradientDescentOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the gradient descent algorithm.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientDescentOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate, use_locking=False, name='GradientDescent')\n",
      "     |      Construct a new gradient descent optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A Tensor or a floating point value.  The learning\n",
      "     |          rate to use.\n",
      "     |        use_locking: If True use locks for update operations.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients. Defaults to \"GradientDescent\".\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class Int64List(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      Int64List\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  value\n",
      "     |      Magic attribute generated for \"value\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  VALUE_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\x08': (<function DecodeRepeatedField>, None), '\\...\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class LooperThread(threading.Thread)\n",
      "     |  A thread that runs code repeatedly, optionally on a timer.\n",
      "     |  \n",
      "     |  This thread class is intended to be used with a `Coordinator`.  It repeatedly\n",
      "     |  runs code specified either as `target` and `args` or by the `run_loop()`\n",
      "     |  method.\n",
      "     |  \n",
      "     |  Before each run the thread checks if the coordinator has requested stop.  In\n",
      "     |  that case the looper thread terminates immediately.\n",
      "     |  \n",
      "     |  If the code being run raises an exception, that exception is reported to the\n",
      "     |  coordinator and the thread terminates.  The coordinator will then request all\n",
      "     |  the other threads it coordinates to stop.\n",
      "     |  \n",
      "     |  You typically pass looper threads to the supervisor `Join()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LooperThread\n",
      "     |      threading.Thread\n",
      "     |      threading._Verbose\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, coord, timer_interval_secs, target=None, args=None, kwargs=None)\n",
      "     |      Create a LooperThread.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        coord: A Coordinator.\n",
      "     |        timer_interval_secs: Time boundaries at which to call Run(), or None\n",
      "     |          if it should be called back to back.\n",
      "     |        target: Optional callable object that will be executed in the thread.\n",
      "     |        args: Optional arguments to pass to `target` when calling it.\n",
      "     |        kwargs: Optional keyword arguments to pass to `target` when calling it.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If one of the arguments is invalid.\n",
      "     |  \n",
      "     |  run(self)\n",
      "     |  \n",
      "     |  run_loop(self)\n",
      "     |      Called at 'timer_interval_secs' boundaries.\n",
      "     |  \n",
      "     |  start_loop(self)\n",
      "     |      Called when the thread starts.\n",
      "     |  \n",
      "     |  stop_loop(self)\n",
      "     |      Called when the thread stops.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  loop(coord, timer_interval_secs, target, args=None, kwargs=None)\n",
      "     |      Start a LooperThread that calls a function periodically.\n",
      "     |      \n",
      "     |      If `timer_interval_secs` is None the thread calls `target(args)`\n",
      "     |      repeatedly.  Otherwise `target(args)` is called every `timer_interval_secs`\n",
      "     |      seconds.  The thread terminates when a stop of the coordinator is\n",
      "     |      requested.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        coord: A Coordinator.\n",
      "     |        timer_interval_secs: Number. Time boundaries at which to call `target`.\n",
      "     |        target: A callable object.\n",
      "     |        args: Optional arguments to pass to `target` when calling it.\n",
      "     |        kwargs: Optional keyword arguments to pass to `target` when calling it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The started thread.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from threading.Thread:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  getName(self)\n",
      "     |  \n",
      "     |  isAlive(self)\n",
      "     |      Return whether the thread is alive.\n",
      "     |      \n",
      "     |      This method returns True just before the run() method starts until just\n",
      "     |      after the run() method terminates. The module function enumerate()\n",
      "     |      returns a list of all alive threads.\n",
      "     |  \n",
      "     |  isDaemon(self)\n",
      "     |  \n",
      "     |  is_alive = isAlive(self)\n",
      "     |      Return whether the thread is alive.\n",
      "     |      \n",
      "     |      This method returns True just before the run() method starts until just\n",
      "     |      after the run() method terminates. The module function enumerate()\n",
      "     |      returns a list of all alive threads.\n",
      "     |  \n",
      "     |  join(self, timeout=None)\n",
      "     |      Wait until the thread terminates.\n",
      "     |      \n",
      "     |      This blocks the calling thread until the thread whose join() method is\n",
      "     |      called terminates -- either normally or through an unhandled exception\n",
      "     |      or until the optional timeout occurs.\n",
      "     |      \n",
      "     |      When the timeout argument is present and not None, it should be a\n",
      "     |      floating point number specifying a timeout for the operation in seconds\n",
      "     |      (or fractions thereof). As join() always returns None, you must call\n",
      "     |      isAlive() after join() to decide whether a timeout happened -- if the\n",
      "     |      thread is still alive, the join() call timed out.\n",
      "     |      \n",
      "     |      When the timeout argument is not present or None, the operation will\n",
      "     |      block until the thread terminates.\n",
      "     |      \n",
      "     |      A thread can be join()ed many times.\n",
      "     |      \n",
      "     |      join() raises a RuntimeError if an attempt is made to join the current\n",
      "     |      thread as that would cause a deadlock. It is also an error to join() a\n",
      "     |      thread before it has been started and attempts to do so raises the same\n",
      "     |      exception.\n",
      "     |  \n",
      "     |  setDaemon(self, daemonic)\n",
      "     |  \n",
      "     |  setName(self, name)\n",
      "     |  \n",
      "     |  start(self)\n",
      "     |      Start the thread's activity.\n",
      "     |      \n",
      "     |      It must be called at most once per thread object. It arranges for the\n",
      "     |      object's run() method to be invoked in a separate thread of control.\n",
      "     |      \n",
      "     |      This method will raise a RuntimeError if called more than once on the\n",
      "     |      same thread object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from threading.Thread:\n",
      "     |  \n",
      "     |  daemon\n",
      "     |      A boolean value indicating whether this thread is a daemon thread (True) or not (False).\n",
      "     |      \n",
      "     |      This must be set before start() is called, otherwise RuntimeError is\n",
      "     |      raised. Its initial value is inherited from the creating thread; the\n",
      "     |      main thread is not a daemon thread and therefore all threads created in\n",
      "     |      the main thread default to daemon = False.\n",
      "     |      \n",
      "     |      The entire Python program exits when no alive non-daemon threads are\n",
      "     |      left.\n",
      "     |  \n",
      "     |  ident\n",
      "     |      Thread identifier of this thread or None if it has not been started.\n",
      "     |      \n",
      "     |      This is a nonzero integer. See the thread.get_ident() function. Thread\n",
      "     |      identifiers may be recycled when a thread exits and another thread is\n",
      "     |      created. The identifier is available even after the thread has exited.\n",
      "     |  \n",
      "     |  name\n",
      "     |      A string used for identification purposes only.\n",
      "     |      \n",
      "     |      It has no semantics. Multiple threads may be given the same name. The\n",
      "     |      initial name is set by the constructor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from threading._Verbose:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MomentumOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the Momentum algorithm.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MomentumOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False)\n",
      "     |      Construct a new Momentum optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A `Tensor` or a floating point value.  The learning rate.\n",
      "     |        momentum: A `Tensor` or a floating point value.  The momentum.\n",
      "     |        use_locking: If `True` use locks for update operations.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients.  Defaults to \"Momentum\".\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class Optimizer(__builtin__.object)\n",
      "     |  Base class for optimizers.\n",
      "     |  \n",
      "     |  This class defines the API to add Ops to train a model.  You never use this\n",
      "     |  class directly, but instead instantiate one of its subclasses such as\n",
      "     |  `GradientDescentOptimizer`, `AdagradOptimizer`, or `MomentumOptimizer`.\n",
      "     |  \n",
      "     |  ### Usage\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Create an optimizer with the desired parameters.\n",
      "     |  opt = GradientDescentOptimizer(learning_rate=0.1)\n",
      "     |  # Add Ops to the graph to minimize a cost by updating a list of variables.\n",
      "     |  # \"cost\" is a Tensor, and the list of variables contains tf.Variable\n",
      "     |  # objects.\n",
      "     |  opt_op = opt.minimize(cost, var_list=<list of variables>)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  In the training program you will just have to run the returned Op.\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Execute opt_op to do one step of training:\n",
      "     |  opt_op.run()\n",
      "     |  ```\n",
      "     |  \n",
      "     |  ### Processing gradients before applying them.\n",
      "     |  \n",
      "     |  Calling `minimize()` takes care of both computing the gradients and\n",
      "     |  applying them to the variables.  If you want to process the gradients\n",
      "     |  before applying them you can instead use the optimizer in three steps:\n",
      "     |  \n",
      "     |  1.  Compute the gradients with `compute_gradients()`.\n",
      "     |  2.  Process the gradients as you wish.\n",
      "     |  3.  Apply the processed gradients with `apply_gradients()`.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Create an optimizer.\n",
      "     |  opt = GradientDescentOptimizer(learning_rate=0.1)\n",
      "     |  \n",
      "     |  # Compute the gradients for a list of variables.\n",
      "     |  grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
      "     |  \n",
      "     |  # grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
      "     |  # need to the 'gradient' part, for example cap them, etc.\n",
      "     |  capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
      "     |  \n",
      "     |  # Ask the optimizer to apply the capped gradients.\n",
      "     |  opt.apply_gradients(capped_grads_and_vars)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  @@minimize\n",
      "     |  @@compute_gradients\n",
      "     |  @@apply_gradients\n",
      "     |  \n",
      "     |  ### Gating Gradients\n",
      "     |  \n",
      "     |  Both `minimize()` and `compute_gradients()` accept a `gate_gradient` argument\n",
      "     |  that controls the degree of parallelism during the application of the\n",
      "     |  gradients.\n",
      "     |  \n",
      "     |  The possible values are: `GATE_NONE`, `GATE_OP`, and `GATE_GRAPH`.\n",
      "     |  \n",
      "     |  <b>`GATE_NONE`</b>: Compute and apply gradients in parallel.  This provides\n",
      "     |  the maximum parallelism in execution, at the cost of some non-reproducibility\n",
      "     |  in the results.  For example the two gradients of `matmul` depend on the input\n",
      "     |  values: With `GATE_NONE` one of the gradients could be applied to one of the\n",
      "     |  inputs _before_ the other gradient is computed resulting in non-reproducible\n",
      "     |  results.\n",
      "     |  \n",
      "     |  <b>`GATE_OP`</b>: For each Op, make sure all gradients are computed before\n",
      "     |  they are used.  This prevents race conditions for Ops that generate gradients\n",
      "     |  for multiple inputs where the gradients depend on the inputs.\n",
      "     |  \n",
      "     |  <b>`GATE_GRAPH`</b>: Make sure all gradients for all variables are computed\n",
      "     |  before any one of them is used.  This provides the least parallelism but can\n",
      "     |  be useful if you want to process all gradients before applying any of them.\n",
      "     |  \n",
      "     |  ### Slots\n",
      "     |  \n",
      "     |  Some optimizer subclasses, such as `MomentumOptimizer` and `AdagradOptimizer`\n",
      "     |  allocate and manage additional variables associated with the variables to\n",
      "     |  train.  These are called <i>Slots</i>.  Slots have names and you can ask the\n",
      "     |  optimizer for the names of the slots that it uses.  Once you have a slot name\n",
      "     |  you can ask the optimizer for the variable it created to hold the slot value.\n",
      "     |  \n",
      "     |  This can be useful if you want to log debug a training algorithm, report stats\n",
      "     |  about the slots, etc.\n",
      "     |  \n",
      "     |  @@get_slot_names\n",
      "     |  @@get_slot\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, use_locking, name)\n",
      "     |      Create a new Optimizer.\n",
      "     |      \n",
      "     |      This must be called by the constructors of subclasses.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        use_locking: Bool. If True apply use locks to prevent concurrent updates\n",
      "     |          to variables.\n",
      "     |        name: A non-empty string.  The name to use for accumulators created\n",
      "     |          for the optimizer.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If name is malformed.\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class QueueRunner(__builtin__.object)\n",
      "     |  Holds a list of enqueue operations for a queue, each to be run in a thread.\n",
      "     |  \n",
      "     |  Queues are a convenient TensorFlow mechanism to compute tensors\n",
      "     |  asynchronously using multiple threads. For example in the canonical 'Input\n",
      "     |  Reader' setup one set of threads generates filenames in a queue; a second set\n",
      "     |  of threads read records from the files, processes them, and enqueues tensors\n",
      "     |  on a second queue; a third set of threads dequeues these input records to\n",
      "     |  construct batches and runs them through training operations.\n",
      "     |  \n",
      "     |  There are several delicate issues when running multiple threads that way:\n",
      "     |  closing the queues in sequence as the input is exhausted, correctly catching\n",
      "     |  and reporting exceptions, etc.\n",
      "     |  \n",
      "     |  The `QueueRunner`, combined with the `Coordinator`, helps handle these issues.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, queue=None, enqueue_ops=None, close_op=None, cancel_op=None, queue_runner_def=None)\n",
      "     |      Create a QueueRunner.\n",
      "     |      \n",
      "     |      On construction the `QueueRunner` adds an op to close the queue.  That op\n",
      "     |      will be run if the enqueue ops raise exceptions.\n",
      "     |      \n",
      "     |      When you later call the `create_threads()` method, the `QueueRunner` will\n",
      "     |      create one thread for each op in `enqueue_ops`.  Each thread will run its\n",
      "     |      enqueue op in parallel with the other threads.  The enqueue ops do not have\n",
      "     |      to all be the same op, but it is expected that they all enqueue tensors in\n",
      "     |      `queue`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        queue: A `Queue`.\n",
      "     |        enqueue_ops: List of enqueue ops to run in threads later.\n",
      "     |        close_op: Op to close the queue. Pending enqueue ops are preserved.\n",
      "     |        cancel_op: Op to close the queue and cancel pending enqueue ops.\n",
      "     |        queue_runner_def: Optional `QueueRunnerDef` protocol buffer. If specified,\n",
      "     |          recreates the QueueRunner from its contents. `queue_runner_def` and the\n",
      "     |          other arguments are mutually exclusive.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If both `queue_runner_def` and `queue` are both specified.\n",
      "     |        ValueError: If `queue` or `enqueue_ops` are not provided when not\n",
      "     |          restoring from `queue_runner_def`.\n",
      "     |  \n",
      "     |  create_threads(self, sess, coord=None, daemon=False, start=False)\n",
      "     |      Create threads to run the enqueue ops.\n",
      "     |      \n",
      "     |      This method requires a session in which the graph was launched.  It creates\n",
      "     |      a list of threads, optionally starting them.  There is one thread for each\n",
      "     |      op passed in `enqueue_ops`.\n",
      "     |      \n",
      "     |      The `coord` argument is an optional coordinator, that the threads will use\n",
      "     |      to terminate together and report exceptions.  If a coordinator is given,\n",
      "     |      this method starts an additional thread to close the queue when the\n",
      "     |      coordinator requests a stop.\n",
      "     |      \n",
      "     |      This method may be called again as long as all threads from a previous call\n",
      "     |      have stopped.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A `Session`.\n",
      "     |        coord: Optional `Coordinator` object for reporting errors and checking\n",
      "     |          stop conditions.\n",
      "     |        daemon: Boolean.  If `True` make the threads daemon threads.\n",
      "     |        start: Boolean.  If `True` starts the threads.  If `False` the\n",
      "     |          caller must call the `start()` method of the returned threads.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of threads.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        RuntimeError: If threads from a previous call to `create_threads()` are\n",
      "     |        still running.\n",
      "     |  \n",
      "     |  to_proto(self)\n",
      "     |      Converts this `QueueRunner` to a `QueueRunnerDef` protocol buffer.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `QueueRunnerDef` protocol buffer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  from_proto(queue_runner_def)\n",
      "     |      Returns a `QueueRunner` object created from `queue_runner_def`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  cancel_op\n",
      "     |  \n",
      "     |  close_op\n",
      "     |  \n",
      "     |  enqueue_ops\n",
      "     |  \n",
      "     |  exceptions_raised\n",
      "     |      Exceptions raised but not handled by the `QueueRunner` threads.\n",
      "     |      \n",
      "     |      Exceptions raised in queue runner threads are handled in one of two ways\n",
      "     |      depending on whether or not a `Coordinator` was passed to\n",
      "     |      `create_threads()`:\n",
      "     |      \n",
      "     |      * With a `Coordinator`, exceptions are reported to the coordinator and\n",
      "     |        forgotten by the `QueueRunner`.\n",
      "     |      * Without a `Coordinator`, exceptions are captured by the `QueueRunner` and\n",
      "     |        made available in this `exceptions_raised` property.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of Python `Exception` objects.  The list is empty if no exception\n",
      "     |        was captured.  (No exceptions are captured when using a Coordinator.)\n",
      "     |  \n",
      "     |  name\n",
      "     |      The string name of the underlying Queue.\n",
      "     |  \n",
      "     |  queue\n",
      "    \n",
      "    class RMSPropOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      "     |  Optimizer that implements the RMSProp algorithm.\n",
      "     |  \n",
      "     |  See the [paper]\n",
      "     |  (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RMSPropOptimizer\n",
      "     |      tensorflow.python.training.optimizer.Optimizer\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, name='RMSProp')\n",
      "     |      Construct a new RMSProp optimizer.\n",
      "     |      \n",
      "     |      Note that in dense implement of this algorithm, m_t and v_t will \n",
      "     |      update even if g is zero, but in sparse implement, m_t and v_t \n",
      "     |      will not update in iterations g is zero.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      "     |        decay: Discounting factor for the history/coming gradient\n",
      "     |        momentum: A scalar tensor.\n",
      "     |        epsilon: Small value to avoid zero denominator.\n",
      "     |        use_locking: If True use locks for update operation.\n",
      "     |        name: Optional name prefix for the operations created when applying\n",
      "     |          gradients. Defaults to \"RMSProp\".\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      "     |      Apply gradients to variables.\n",
      "     |      \n",
      "     |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      "     |      applies gradients.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      "     |          `compute_gradients()`.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        name: Optional name for the returned operation.  Default to the\n",
      "     |          name passed to the `Optimizer` constructor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An `Operation` that applies the specified gradients. If `global_step`\n",
      "     |        was not None, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `grads_and_vars` is malformed.\n",
      "     |        ValueError: If none of the variables have gradients.\n",
      "     |  \n",
      "     |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n",
      "     |      Compute gradients of `loss` for the variables in `var_list`.\n",
      "     |      \n",
      "     |      This is the first part of `minimize()`.  It returns a list\n",
      "     |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      "     |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      "     |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      "     |      given variable.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A Tensor containing the value to minimize.\n",
      "     |        var_list: Optional list of tf.Variable to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of (gradient, variable) pairs.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      "     |        ValueError: If some arguments are invalid.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  get_slot(self, var, name)\n",
      "     |      Return a slot named `name` created for `var` by the Optimizer.\n",
      "     |      \n",
      "     |      Some `Optimizer` subclasses use additional variables.  For example\n",
      "     |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      "     |      gives access to these `Variable` objects if for some reason you need them.\n",
      "     |      \n",
      "     |      Use `get_slot_names()` to get the list of slot names created by the\n",
      "     |      `Optimizer`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      "     |        name: A string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      "     |  \n",
      "     |  get_slot_names(self)\n",
      "     |      Return a list of the names of slots created by the `Optimizer`.\n",
      "     |      \n",
      "     |      See `get_slot()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.\n",
      "     |  \n",
      "     |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
      "     |      Add operations to minimize `loss` by updating `var_list`.\n",
      "     |      \n",
      "     |      This method simply combines calls `compute_gradients()` and\n",
      "     |      `apply_gradients()`. If you want to process the gradient before applying\n",
      "     |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "     |      of using this function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        loss: A `Tensor` containing the value to minimize.\n",
      "     |        global_step: Optional `Variable` to increment by one after the\n",
      "     |          variables have been updated.\n",
      "     |        var_list: Optional list of `Variable` objects to update to minimize\n",
      "     |          `loss`.  Defaults to the list of variables collected in the graph\n",
      "     |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "     |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      "     |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "     |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      "     |          Valid values are defined in the class `AggregationMethod`.\n",
      "     |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "     |          the corresponding op.\n",
      "     |        name: Optional name for the returned operation.\n",
      "     |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "     |        was not `None`, that operation also increments `global_step`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If some of the variables are not `Variable` objects.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  GATE_GRAPH = 2\n",
      "     |  \n",
      "     |  GATE_NONE = 0\n",
      "     |  \n",
      "     |  GATE_OP = 1\n",
      "    \n",
      "    class Saver(__builtin__.object)\n",
      "     |  Saves and restores variables.\n",
      "     |  \n",
      "     |  See [Variables](../../how_tos/variables/index.md)\n",
      "     |  for an overview of variables, saving and restoring.\n",
      "     |  \n",
      "     |  The `Saver` class adds ops to save and restore variables to and from\n",
      "     |  *checkpoints*.  It also provides convenience methods to run these ops.\n",
      "     |  \n",
      "     |  Checkpoints are binary files in a proprietary format which map variable names\n",
      "     |  to tensor values.  The best way to examine the contents of a checkpoint is to\n",
      "     |  load it using a `Saver`.\n",
      "     |  \n",
      "     |  Savers can automatically number checkpoint filenames with a provided counter.\n",
      "     |  This lets you keep multiple checkpoints at different steps while training a\n",
      "     |  model.  For example you can number the checkpoint filenames with the training\n",
      "     |  step number.  To avoid filling up disks, savers manage checkpoint files\n",
      "     |  automatically. For example, they can keep only the N most recent files, or\n",
      "     |  one checkpoint for every N hours of training.\n",
      "     |  \n",
      "     |  You number checkpoint filenames by passing a value to the optional\n",
      "     |  `global_step` argument to `save()`:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n",
      "     |  ...\n",
      "     |  saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Additionally, optional arguments to the `Saver()` constructor let you control\n",
      "     |  the proliferation of checkpoint files on disk:\n",
      "     |  \n",
      "     |  * `max_to_keep` indicates the maximum number of recent checkpoint files to\n",
      "     |    keep.  As new files are created, older files are deleted.  If None or 0,\n",
      "     |    all checkpoint files are kept.  Defaults to 5 (that is, the 5 most recent\n",
      "     |    checkpoint files are kept.)\n",
      "     |  \n",
      "     |  * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent\n",
      "     |    `max_to_keep` checkpoint files, you might want to keep one checkpoint file\n",
      "     |    for every N hours of training.  This can be useful if you want to later\n",
      "     |    analyze how a model progressed during a long training session.  For\n",
      "     |    example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep\n",
      "     |    one checkpoint file for every 2 hours of training.  The default value of\n",
      "     |    10,000 hours effectively disables the feature.\n",
      "     |  \n",
      "     |  Note that you still have to call the `save()` method to save the model.\n",
      "     |  Passing these arguments to the constructor will not save variables\n",
      "     |  automatically for you.\n",
      "     |  \n",
      "     |  A training program that saves regularly looks like:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  ...\n",
      "     |  # Create a saver.\n",
      "     |  saver = tf.train.Saver(...variables...)\n",
      "     |  # Launch the graph and train, saving the model every 1,000 steps.\n",
      "     |  sess = tf.Session()\n",
      "     |  for step in xrange(1000000):\n",
      "     |      sess.run(..training_op..)\n",
      "     |      if step % 1000 == 0:\n",
      "     |          # Append the step number to the checkpoint name:\n",
      "     |          saver.save(sess, 'my-model', global_step=step)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  In addition to checkpoint files, savers keep a protocol buffer on disk with\n",
      "     |  the list of recent checkpoints. This is used to manage numbered checkpoint\n",
      "     |  files and by `latest_checkpoint()`, which makes it easy to discover the path\n",
      "     |  to the most recent checkpoint. That protocol buffer is stored in a file named\n",
      "     |  'checkpoint' next to the checkpoint files.\n",
      "     |  \n",
      "     |  If you create several savers, you can specify a different filename for the\n",
      "     |  protocol buffer file in the call to `save()`.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  @@save\n",
      "     |  @@restore\n",
      "     |  \n",
      "     |  Other utility methods.\n",
      "     |  \n",
      "     |  @@last_checkpoints\n",
      "     |  @@set_last_checkpoints\n",
      "     |  @@as_saver_def\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None)\n",
      "     |      Creates a `Saver`.\n",
      "     |      \n",
      "     |      The constructor adds ops to save and restore variables.\n",
      "     |      \n",
      "     |      `var_list` specifies the variables that will be saved and restored. It can\n",
      "     |      be passed as a `dict` or a list:\n",
      "     |      \n",
      "     |      * A `dict` of names to variables: The keys are the names that will be\n",
      "     |        used to save or restore the variables in the checkpoint files.\n",
      "     |      * A list of variables: The variables will be keyed with their op name in\n",
      "     |        the checkpoint files.\n",
      "     |      \n",
      "     |      For example:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      v1 = tf.Variable(..., name='v1')\n",
      "     |      v2 = tf.Variable(..., name='v2')\n",
      "     |      \n",
      "     |      # Pass the variables as a dict:\n",
      "     |      saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
      "     |      \n",
      "     |      # Or pass them as a list.\n",
      "     |      saver = tf.train.Saver([v1, v2])\n",
      "     |      # Passing a list is equivalent to passing a dict with the variable op names\n",
      "     |      # as keys:\n",
      "     |      saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
      "     |      ```\n",
      "     |      \n",
      "     |      The optional `reshape` argument, if `True`, allows restoring a variable from\n",
      "     |      a save file where the variable had a different shape, but the same number\n",
      "     |      of elements and type.  This is useful if you have reshaped a variable and\n",
      "     |      want to reload it from an older checkpoint.\n",
      "     |      \n",
      "     |      The optional `sharded` argument, if `True`, instructs the saver to shard\n",
      "     |      checkpoints per device.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        var_list: A list of `Variable` objects or a dictionary mapping names to\n",
      "     |          variables.  If `None`, defaults to the list of all variables.\n",
      "     |        reshape: If `True`, allows restoring parameters from a checkpoint\n",
      "     |          where the variables have a different shape.\n",
      "     |        sharded: If `True`, shard the checkpoints, one per device.\n",
      "     |        max_to_keep: Maximum number of recent checkpoints to keep.\n",
      "     |          Defaults to 5.\n",
      "     |        keep_checkpoint_every_n_hours: How often to keep checkpoints.\n",
      "     |          Defaults to 10,000 hours.\n",
      "     |        name: String.  Optional name to use as a prefix when adding operations.\n",
      "     |        restore_sequentially: A `Bool`, which if true, causes restore of different\n",
      "     |          variables to happen sequentially within each device.  This can lower\n",
      "     |          memory usage when restoring very large models.\n",
      "     |        saver_def: Optional `SaverDef` proto to use instead of running the\n",
      "     |          builder. This is only useful for specialty code that wants to recreate\n",
      "     |          a `Saver` object for a previously built `Graph` that had a `Saver`.\n",
      "     |          The `saver_def` proto should be the one returned by the\n",
      "     |          `as_saver_def()` call of the `Saver` that was created for that `Graph`.\n",
      "     |        builder: Optional `SaverBuilder` to use if a `saver_def` was not provided.\n",
      "     |          Defaults to `BaseSaverBuilder()`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `var_list` is invalid.\n",
      "     |        ValueError: If any of the keys or values in `var_list` are not unique.\n",
      "     |  \n",
      "     |  as_saver_def(self)\n",
      "     |      Generates a `SaverDef` representation of this saver.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `SaverDef` proto.\n",
      "     |  \n",
      "     |  export_meta_graph(self, filename=None, collection_list=None, as_text=False)\n",
      "     |      Writes `MetaGraphDef` to save_path/filename.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        filename: Optional meta_graph filename including the path.\n",
      "     |        collection_list: List of string keys to collect.\n",
      "     |        as_text: If `True`, writes the meta_graph as an ASCII proto.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `MetaGraphDef` proto.\n",
      "     |  \n",
      "     |  restore(self, sess, save_path)\n",
      "     |      Restores previously saved variables.\n",
      "     |      \n",
      "     |      This method runs the ops added by the constructor for restoring variables.\n",
      "     |      It requires a session in which the graph was launched.  The variables to\n",
      "     |      restore do not have to have been initialized, as restoring is itself a way\n",
      "     |      to initialize variables.\n",
      "     |      \n",
      "     |      The `save_path` argument is typically a value previously returned from a\n",
      "     |      `save()` call, or a call to `latest_checkpoint()`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A `Session` to use to restore the parameters.\n",
      "     |        save_path: Path where parameters were previously saved.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the given `save_path` does not point to a file.\n",
      "     |  \n",
      "     |  save(self, sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True)\n",
      "     |      Saves variables.\n",
      "     |      \n",
      "     |      This method runs the ops added by the constructor for saving variables.\n",
      "     |      It requires a session in which the graph was launched.  The variables to\n",
      "     |      save must also have been initialized.\n",
      "     |      \n",
      "     |      The method returns the path of the newly created checkpoint file.  This\n",
      "     |      path can be passed directly to a call to `restore()`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A Session to use to save the variables.\n",
      "     |        save_path: String.  Path to the checkpoint filename.  If the saver is\n",
      "     |          `sharded`, this is the prefix of the sharded checkpoint filename.\n",
      "     |        global_step: If provided the global step number is appended to\n",
      "     |          `save_path` to create the checkpoint filename. The optional argument\n",
      "     |          can be a `Tensor`, a `Tensor` name or an integer.\n",
      "     |        latest_filename: Optional name for the protocol buffer file that will\n",
      "     |          contains the list of most recent checkpoint filenames.  That file,\n",
      "     |          kept in the same directory as the checkpoint files, is automatically\n",
      "     |          managed by the saver to keep track of recent checkpoints.  Defaults to\n",
      "     |          'checkpoint'.\n",
      "     |        meta_graph_suffix: Suffix for `MetaGraphDef` file. Defaults to 'meta'.\n",
      "     |        write_meta_graph: `Boolean` indicating whether or not to write the meta\n",
      "     |          graph file.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A string: path at which the variables were saved.  If the saver is\n",
      "     |          sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'\n",
      "     |          is the number of shards created.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: If `sess` is not a `Session`.\n",
      "     |        ValueError: If `latest_filename` contains path components, or if it\n",
      "     |          collides with `save_path`.\n",
      "     |  \n",
      "     |  set_last_checkpoints(self, last_checkpoints)\n",
      "     |      DEPRECATED: Use set_last_checkpoints_with_time.\n",
      "     |      \n",
      "     |      Sets the list of old checkpoint filenames.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        last_checkpoints: A list of checkpoint filenames.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        AssertionError: If last_checkpoints is not a list.\n",
      "     |  \n",
      "     |  set_last_checkpoints_with_time(self, last_checkpoints_with_time)\n",
      "     |      Sets the list of old checkpoint filenames and timestamps.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        last_checkpoints_with_time: A list of tuples of checkpoint filenames and\n",
      "     |          timestamps.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        AssertionError: If last_checkpoints_with_time is not a list.\n",
      "     |  \n",
      "     |  to_proto(self)\n",
      "     |      Converts this `Saver` to a `SaverDef` protocol buffer.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `SaverDef` protocol buffer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  from_proto(saver_def)\n",
      "     |      Returns a `Saver` object created from `saver_def`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  last_checkpoints\n",
      "     |      List of not-yet-deleted checkpoint filenames.\n",
      "     |      \n",
      "     |      You can pass any of the returned values to `restore()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of checkpoint filenames, sorted from oldest to newest.\n",
      "    \n",
      "    class SaverDef(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      SaverDef\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  filename_tensor_name\n",
      "     |      Magic attribute generated for \"filename_tensor_name\" proto field.\n",
      "     |  \n",
      "     |  keep_checkpoint_every_n_hours\n",
      "     |      Magic attribute generated for \"keep_checkpoint_every_n_hours\" proto field.\n",
      "     |  \n",
      "     |  max_to_keep\n",
      "     |      Magic attribute generated for \"max_to_keep\" proto field.\n",
      "     |  \n",
      "     |  restore_op_name\n",
      "     |      Magic attribute generated for \"restore_op_name\" proto field.\n",
      "     |  \n",
      "     |  save_tensor_name\n",
      "     |      Magic attribute generated for \"save_tensor_name\" proto field.\n",
      "     |  \n",
      "     |  sharded\n",
      "     |      Magic attribute generated for \"sharded\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FILENAME_TENSOR_NAME_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  KEEP_CHECKPOINT_EVERY_N_HOURS_FIELD_NUMBER = 6\n",
      "     |  \n",
      "     |  MAX_TO_KEEP_FIELD_NUMBER = 4\n",
      "     |  \n",
      "     |  RESTORE_OP_NAME_FIELD_NUMBER = 3\n",
      "     |  \n",
      "     |  SAVE_TENSOR_NAME_FIELD_NUMBER = 2\n",
      "     |  \n",
      "     |  SHARDED_FIELD_NUMBER = 5\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeField>, None), '\\x12': (<fu...\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class SequenceExample(google.protobuf.message.Message)\n",
      "     |  Method resolution order:\n",
      "     |      SequenceExample\n",
      "     |      google.protobuf.message.Message\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ByteSize(self)\n",
      "     |  \n",
      "     |  Clear(self)\n",
      "     |  \n",
      "     |  ClearField(self, field_name)\n",
      "     |  \n",
      "     |  FindInitializationErrors(self)\n",
      "     |      Finds required fields which are not initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of strings.  Each string is a path to an uninitialized field from\n",
      "     |        the top-level message, e.g. \"foo.bar[5].baz\".\n",
      "     |  \n",
      "     |  HasField(self, field_name)\n",
      "     |  \n",
      "     |  IsInitialized(self, errors=None)\n",
      "     |      Checks if all required fields of a message are set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        errors:  A list which, if provided, will be populated with the field\n",
      "     |                 paths of all missing required fields.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True iff the specified message has all required fields set.\n",
      "     |  \n",
      "     |  ListFields(self)\n",
      "     |  \n",
      "     |  MergeFrom(self, msg)\n",
      "     |  \n",
      "     |  MergeFromString(self, serialized)\n",
      "     |  \n",
      "     |  SerializePartialToString(self)\n",
      "     |  \n",
      "     |  SerializeToString(self)\n",
      "     |  \n",
      "     |  SetInParent = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  WhichOneof(self, oneof_name)\n",
      "     |      Returns the name of the currently set field inside a oneof, or None.\n",
      "     |  \n",
      "     |  _InternalParse = InternalParse(self, buffer, pos, end)\n",
      "     |  \n",
      "     |  _InternalSerialize = InternalSerialize(self, write_bytes)\n",
      "     |  \n",
      "     |  _Modified = Modified(self)\n",
      "     |      Sets the _cached_byte_size_dirty bit to true,\n",
      "     |      and propagates this to our listener iff this was a state change.\n",
      "     |  \n",
      "     |  _SetListener = SetListener(self, listener)\n",
      "     |  \n",
      "     |  _UpdateOneofState(self, field)\n",
      "     |      Sets field as the active field in its containing oneof.\n",
      "     |      \n",
      "     |      Will also delete currently active field in the oneof, if it is different\n",
      "     |      from the argument. Does not mark the message as modified.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__ = init(self, **kwargs)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  FromString(s)\n",
      "     |  \n",
      "     |  RegisterExtension(extension_handle)\n",
      "     |      # TODO(robinson): This probably needs to be thread-safe(?)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  _cached_byte_size\n",
      "     |  \n",
      "     |  _cached_byte_size_dirty\n",
      "     |  \n",
      "     |  _fields\n",
      "     |  \n",
      "     |  _is_present_in_parent\n",
      "     |  \n",
      "     |  _listener\n",
      "     |  \n",
      "     |  _listener_for_children\n",
      "     |  \n",
      "     |  _oneofs\n",
      "     |  \n",
      "     |  _unknown_fields\n",
      "     |  \n",
      "     |  context\n",
      "     |      Magic attribute generated for \"context\" proto field.\n",
      "     |  \n",
      "     |  feature_lists\n",
      "     |      Magic attribute generated for \"feature_lists\" proto field.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CONTEXT_FIELD_NUMBER = 1\n",
      "     |  \n",
      "     |  DESCRIPTOR = <google.protobuf.descriptor.Descriptor object>\n",
      "     |  \n",
      "     |  FEATURE_LISTS_FIELD_NUMBER = 2\n",
      "     |  \n",
      "     |  _decoders_by_tag = {'\\n': (<function DecodeField>, None), '\\x12': (<fu...\n",
      "     |  \n",
      "     |  _extensions_by_name = {}\n",
      "     |  \n",
      "     |  _extensions_by_number = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from google.protobuf.message.Message:\n",
      "     |  \n",
      "     |  ClearExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  CopyFrom(self, other_msg)\n",
      "     |      Copies the content of the specified message into the current message.\n",
      "     |      \n",
      "     |      The method clears the current message and then merges the specified\n",
      "     |      message using MergeFrom.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        other_msg: Message to copy into the current one.\n",
      "     |  \n",
      "     |  HasExtension(self, extension_handle)\n",
      "     |  \n",
      "     |  ParseFromString(self, serialized)\n",
      "     |      Parse serialized protocol buffer data into this message.\n",
      "     |      \n",
      "     |      Like MergeFromString(), except we clear the object first and\n",
      "     |      do not return the value that MergeFromString returns.\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo=None)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Support the pickle protocol.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __ne__(self, other_msg)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |      Support the pickle protocol.\n",
      "    \n",
      "    class Server(__builtin__.object)\n",
      "     |  An in-process TensorFlow server, for use in distributed training.\n",
      "     |  \n",
      "     |  A `tf.train.Server` instance encapsulates a set of devices and a\n",
      "     |  [`tf.Session`](../../api_docs/python/client.md#Session) target that\n",
      "     |  can participate in distributed training. A server belongs to a\n",
      "     |  cluster (specified by a [`tf.train.ClusterSpec`](#ClusterSpec)), and\n",
      "     |  corresponds to a particular task in a named job. The server can\n",
      "     |  communicate with any other server in the same cluster.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  @@create_local_server\n",
      "     |  @@target\n",
      "     |  @@server_def\n",
      "     |  \n",
      "     |  @@start\n",
      "     |  @@join\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, server_or_cluster_def, job_name=None, task_index=None, protocol=None, config=None, start=True)\n",
      "     |      Creates a new server with the given definition.\n",
      "     |      \n",
      "     |      The `job_name`, `task_index`, and `protocol` arguments are optional, and\n",
      "     |      override any information provided in `server_or_cluster_def`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        server_or_cluster_def: A `tf.train.ServerDef` or\n",
      "     |          `tf.train.ClusterDef` protocol buffer, or a\n",
      "     |          `tf.train.ClusterSpec` object, describing the server to be\n",
      "     |          created and/or the cluster of which it is a member.\n",
      "     |        job_name: (Optional.) Specifies the name of the job of which the server\n",
      "     |          is a member. Defaults to the value in `server_or_cluster_def`, if\n",
      "     |          specified.\n",
      "     |        task_index: (Optional.) Specifies the task index of the server in its\n",
      "     |          job. Defaults to the value in `server_or_cluster_def`, if specified.\n",
      "     |          Otherwise defaults to 0 if the server's job has only one task.\n",
      "     |        protocol: (Optional.) Specifies the protocol to be used by the server.\n",
      "     |          Acceptable values include `\"grpc\"`. Defaults to the value in\n",
      "     |          `server_or_cluster_def`, if specified. Otherwise defaults to `\"grpc\"`.\n",
      "     |        config: (Options.) A `tf.ConfigProto` that specifies default\n",
      "     |          configuration options for all sessions that run on this server.\n",
      "     |        start: (Optional.) Boolean, indicating whether to start the server\n",
      "     |          after creating it. Defaults to `True`.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        tf.errors.OpError: Or one of its subclasses if an error occurs while\n",
      "     |          creating the TensorFlow server.\n",
      "     |  \n",
      "     |  join(self)\n",
      "     |      Blocks until the server has shut down.\n",
      "     |      \n",
      "     |      This method currently blocks forever.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        tf.errors.OpError: Or one of its subclasses if an error occurs while\n",
      "     |          joining the TensorFlow server.\n",
      "     |  \n",
      "     |  start(self)\n",
      "     |      Starts this server.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        tf.errors.OpError: Or one of its subclasses if an error occurs while\n",
      "     |          starting the TensorFlow server.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  create_local_server(config=None, start=True)\n",
      "     |      Creates a new single-process cluster running on the local host.\n",
      "     |      \n",
      "     |      This method is a convenience wrapper for creating a\n",
      "     |      `tf.train.Server` with a `tf.train.ServerDef` that specifies a\n",
      "     |      single-process cluster containing a single task in a job called\n",
      "     |      `\"local\"`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        config: (Options.) A `tf.ConfigProto` that specifies default\n",
      "     |          configuration options for all sessions that run on this server.\n",
      "     |        start: (Optional.) Boolean, indicating whether to start the server after\n",
      "     |          creating it. Defaults to `True`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A local `tf.train.Server`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  server_def\n",
      "     |      Returns the `tf.train.ServerDef` for this server.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `tf.train.ServerDef` prototocol buffer that describes the configuration\n",
      "     |        of this server.\n",
      "     |  \n",
      "     |  target\n",
      "     |      Returns the target for a `tf.Session` to connect to this server.\n",
      "     |      \n",
      "     |      To create a\n",
      "     |      [`tf.Session`](../../api_docs/python/client.md#Session) that\n",
      "     |      connects to this server, use the following snippet:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      server = tf.train.Server(...)\n",
      "     |      with tf.Session(server.target):\n",
      "     |        # ...\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A string containing a session target for this server.\n",
      "    \n",
      "    class SessionManager(__builtin__.object)\n",
      "     |  Training helper that restores from checkpoint and creates session.\n",
      "     |  \n",
      "     |  This class is a small wrapper that takes care of session creation and\n",
      "     |  checkpoint recovery. It also provides functions that to facilitate\n",
      "     |  coordination among multiple training threads or processes.\n",
      "     |  \n",
      "     |  * Checkpointing trained variables as the training progresses.\n",
      "     |  * Initializing variables on startup, restoring them from the most recent\n",
      "     |    checkpoint after a crash, or wait for checkpoints to become available.\n",
      "     |  \n",
      "     |  ### Usage:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  with tf.Graph().as_default():\n",
      "     |     ...add operations to the graph...\n",
      "     |    # Create a SessionManager that will checkpoint the model in '/tmp/mydir'.\n",
      "     |    sm = SessionManager()\n",
      "     |    sess = sm.prepare_session(master, init_op, saver, checkpoint_dir)\n",
      "     |    # Use the session to train the graph.\n",
      "     |    while True:\n",
      "     |      sess.run(<my_train_op>)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  `prepare_session()` initializes or restores a model. It requires `init_op`\n",
      "     |  and `saver` as an argument.\n",
      "     |  \n",
      "     |  A second process could wait for the model to be ready by doing the following:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  with tf.Graph().as_default():\n",
      "     |     ...add operations to the graph...\n",
      "     |    # Create a SessionManager that will wait for the model to become ready.\n",
      "     |    sm = SessionManager()\n",
      "     |    sess = sm.wait_for_session(master)\n",
      "     |    # Use the session to train the graph.\n",
      "     |    while True:\n",
      "     |      sess.run(<my_train_op>)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  `wait_for_session()` waits for a model to be initialized by other processes.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, local_init_op=None, ready_op=None, graph=None, recovery_wait_secs=30)\n",
      "     |      Creates a SessionManager.\n",
      "     |      \n",
      "     |      The `local_init_op` is an `Operation` that is run always after a new session\n",
      "     |      was created. If `None`, this step is skipped.\n",
      "     |      \n",
      "     |      The `ready_op` is an `Operation` used to check if the model is ready.  The\n",
      "     |      model is considered ready if that operation returns an empty string tensor.\n",
      "     |      If the operation returns non empty string tensor, the elements are\n",
      "     |      concatenated and used to indicate to the user why the model is not ready.\n",
      "     |      \n",
      "     |      If `ready_op` is `None`, the model is not checked for readiness.\n",
      "     |      \n",
      "     |      `recovery_wait_secs` is the number of seconds between checks that\n",
      "     |      the model is ready.  It is used by processes to wait for a model to\n",
      "     |      be initialized or restored.  Defaults to 30 seconds.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        local_init_op: An `Operation` run immediately after session creation.\n",
      "     |           Usually used to initialize tables and local variables.\n",
      "     |        ready_op: An `Operation` to check if the model is initialized.\n",
      "     |        graph: The `Graph` that the model will use.\n",
      "     |        recovery_wait_secs: Seconds between checks for the model to be ready.\n",
      "     |  \n",
      "     |  prepare_session(self, master, init_op=None, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None, init_feed_dict=None, init_fn=None)\n",
      "     |      Creates a `Session`. Makes sure the model is ready to be used.\n",
      "     |      \n",
      "     |      Creates a `Session` on 'master'. If a `saver` object is passed in, and\n",
      "     |      `checkpoint_dir` points to a directory containing valid checkpoint\n",
      "     |      files, then it will try to recover the model from checkpoint. If\n",
      "     |      no checkpoint files are available, and `wait_for_checkpoint` is\n",
      "     |      `True`, then the process would check every `recovery_wait_secs`,\n",
      "     |      up to `max_wait_secs`, for recovery to succeed.\n",
      "     |      \n",
      "     |      If the model cannot be recovered successfully then it is initialized by\n",
      "     |      either running the provided `init_op`, or calling the provided `init_fn`.\n",
      "     |      It is an error if the model cannot be recovered and neither an `init_op`\n",
      "     |      or an `init_fn` are passed.\n",
      "     |      \n",
      "     |      This is a convenient function for the following, with a few error checks\n",
      "     |      added:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      sess, initialized = self.recover_session(master)\n",
      "     |      if not initialized:\n",
      "     |        if init_op:\n",
      "     |          sess.run(init_op, feed_dict=init_feed_dict)\n",
      "     |        if init_fn;\n",
      "     |          init_fn(sess)\n",
      "     |      return sess\n",
      "     |      ```\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        master: `String` representation of the TensorFlow master to use.\n",
      "     |        init_op: Optional `Operation` used to initialize the model.\n",
      "     |        saver: A `Saver` object used to restore a model.\n",
      "     |        checkpoint_dir: Path to the checkpoint files.\n",
      "     |        wait_for_checkpoint: Whether to wait for checkpoint to become available.\n",
      "     |        max_wait_secs: Maximum time to wait for checkpoints to become available.\n",
      "     |        config: Optional `ConfigProto` proto used to configure the session.\n",
      "     |        init_feed_dict: Optional dictionary that maps `Tensor` objects to feed\n",
      "     |          values.  This feed dictionary is passed to the session `run()` call when\n",
      "     |          running the init op.\n",
      "     |        init_fn: Optional callable used to initialize the model. Called after the\n",
      "     |          optional `init_op` is called.  The callable must accept one argument,\n",
      "     |          the session being initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `Session` object that can be used to drive the model.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        RuntimeError: If the model cannot be initialized or recovered.\n",
      "     |  \n",
      "     |  recover_session(self, master, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None)\n",
      "     |      Creates a `Session`, recovering if possible.\n",
      "     |      \n",
      "     |      Creates a new session on 'master'.  If the session is not initialized\n",
      "     |      and can be recovered from a checkpoint, recover it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        master: `String` representation of the TensorFlow master to use.\n",
      "     |        saver: A `Saver` object used to restore a model.\n",
      "     |        checkpoint_dir: Path to the checkpoint files.\n",
      "     |        wait_for_checkpoint: Whether to wait for checkpoint to become available.\n",
      "     |        max_wait_secs: Maximum time to wait for checkpoints to become available.\n",
      "     |        config: Optional `ConfigProto` proto used to configure the session.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A pair (sess, initialized) where 'initialized' is `True` if\n",
      "     |        the session could be recovered, `False` otherwise.\n",
      "     |  \n",
      "     |  wait_for_session(self, master, config=None, max_wait_secs=inf)\n",
      "     |      Creates a new `Session` and waits for model to be ready.\n",
      "     |      \n",
      "     |      Creates a new `Session` on 'master'.  Waits for the model to be\n",
      "     |      initialized or recovered from a checkpoint.  It's expected that\n",
      "     |      another thread or process will make the model ready, and that this\n",
      "     |      is intended to be used by threads/processes that participate in a\n",
      "     |      distributed training configuration where a different thread/process\n",
      "     |      is responsible for initializing or recovering the model being trained.\n",
      "     |      \n",
      "     |      NB: The amount of time this method waits for the session is bounded\n",
      "     |      by max_wait_secs. By default, this function will wait indefinitely.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        master: `String` representation of the TensorFlow master to use.\n",
      "     |        config: Optional ConfigProto proto used to configure the session.\n",
      "     |        max_wait_secs: Maximum time to wait for the session to become available.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `Session`. May be None if the operation exceeds the timeout\n",
      "     |        specified by config.operation_timeout_in_ms.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        tf.DeadlineExceededError: if the session is not available after\n",
      "     |          max_wait_secs.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SummaryWriter(__builtin__.object)\n",
      "     |  Writes `Summary` protocol buffers to event files.\n",
      "     |  \n",
      "     |  The `SummaryWriter` class provides a mechanism to create an event file in a\n",
      "     |  given directory and add summaries and events to it. The class updates the\n",
      "     |  file contents asynchronously. This allows a training program to call methods\n",
      "     |  to add data to the file directly from the training loop, without slowing down\n",
      "     |  training.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  \n",
      "     |  @@add_summary\n",
      "     |  @@add_session_log\n",
      "     |  @@add_event\n",
      "     |  @@add_graph\n",
      "     |  @@add_run_metadata\n",
      "     |  \n",
      "     |  @@flush\n",
      "     |  @@close\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None)\n",
      "     |      Creates a `SummaryWriter` and an event file.\n",
      "     |      \n",
      "     |      On construction the summary writer creates a new event file in `logdir`.\n",
      "     |      This event file will contain `Event` protocol buffers constructed when you\n",
      "     |      call one of the following functions: `add_summary()`, `add_session_log()`,\n",
      "     |      `add_event()`, or `add_graph()`.\n",
      "     |      \n",
      "     |      If you pass a `Graph` to the constructor it is added to\n",
      "     |      the event file. (This is equivalent to calling `add_graph()` later).\n",
      "     |      \n",
      "     |      TensorBoard will pick the graph from the file and display it graphically so\n",
      "     |      you can interactively explore the graph you built. You will usually pass\n",
      "     |      the graph from the session in which you launched it:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      ...create a graph...\n",
      "     |      # Launch the graph in a session.\n",
      "     |      sess = tf.Session()\n",
      "     |      # Create a summary writer, add the 'graph' to the event file.\n",
      "     |      writer = tf.train.SummaryWriter(<some-directory>, sess.graph)\n",
      "     |      ```\n",
      "     |      \n",
      "     |      The other arguments to the constructor control the asynchronous writes to\n",
      "     |      the event file:\n",
      "     |      \n",
      "     |      *  `flush_secs`: How often, in seconds, to flush the added summaries\n",
      "     |         and events to disk.\n",
      "     |      *  `max_queue`: Maximum number of summaries or events pending to be\n",
      "     |         written to disk before one of the 'add' calls block.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        logdir: A string. Directory where event file will be written.\n",
      "     |        graph: A `Graph` object, such as `sess.graph`.\n",
      "     |        max_queue: Integer. Size of the queue for pending events and summaries.\n",
      "     |        flush_secs: Number. How often, in seconds, to flush the\n",
      "     |          pending events and summaries to disk.\n",
      "     |        graph_def: DEPRECATED: Use the `graph` argument instead.\n",
      "     |  \n",
      "     |  add_event(self, event)\n",
      "     |      Adds an event to the event file.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        event: An `Event` protocol buffer.\n",
      "     |  \n",
      "     |  add_graph(self, graph, global_step=None, graph_def=None)\n",
      "     |      Adds a `Graph` to the event file.\n",
      "     |      \n",
      "     |      The graph described by the protocol buffer will be displayed by\n",
      "     |      TensorBoard. Most users pass a graph in the constructor instead.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        graph: A `Graph` object, such as `sess.graph`.\n",
      "     |        global_step: Number. Optional global step counter to record with the\n",
      "     |          graph.\n",
      "     |        graph_def: DEPRECATED. Use the `graph` parameter instead.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If both graph and graph_def are passed to the method.\n",
      "     |  \n",
      "     |  add_run_metadata(self, run_metadata, tag, global_step=None)\n",
      "     |      Adds a metadata information for a single session.run() call.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        run_metadata: A `RunMetadata` protobuf object.\n",
      "     |        tag: The tag name for this metadata.\n",
      "     |        global_step: Number. Optional global step counter to record with the\n",
      "     |          StepStats.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the provided tag was already used for this type of event.\n",
      "     |  \n",
      "     |  add_session_log(self, session_log, global_step=None)\n",
      "     |      Adds a `SessionLog` protocol buffer to the event file.\n",
      "     |      \n",
      "     |      This method wraps the provided session in an `Event` procotol buffer\n",
      "     |      and adds it to the event file.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        session_log: A `SessionLog` protocol buffer.\n",
      "     |        global_step: Number. Optional global step value to record with the\n",
      "     |          summary.\n",
      "     |  \n",
      "     |  add_summary(self, summary, global_step=None)\n",
      "     |      Adds a `Summary` protocol buffer to the event file.\n",
      "     |      \n",
      "     |      This method wraps the provided summary in an `Event` protocol buffer\n",
      "     |      and adds it to the event file.\n",
      "     |      \n",
      "     |      You can pass the result of evaluating any summary op, using\n",
      "     |      [`Session.run()`](client.md#Session.run) or\n",
      "     |      [`Tensor.eval()`](framework.md#Tensor.eval), to this\n",
      "     |      function. Alternatively, you can pass a `tf.Summary` protocol\n",
      "     |      buffer that you populate with your own data. The latter is\n",
      "     |      commonly done to report evaluation results in event files.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        summary: A `Summary` protocol buffer, optionally serialized as a string.\n",
      "     |        global_step: Number. Optional global step value to record with the\n",
      "     |          summary.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Flushes the event file to disk and close the file.\n",
      "     |      \n",
      "     |      Call this method when you do not need the summary writer anymore.\n",
      "     |  \n",
      "     |  flush(self)\n",
      "     |      Flushes the event file to disk.\n",
      "     |      \n",
      "     |      Call this method to make sure that all pending events have been written to\n",
      "     |      disk.\n",
      "     |  \n",
      "     |  reopen(self)\n",
      "     |      Reopens the summary writer.\n",
      "     |      \n",
      "     |      Can be called after `close()` to add more events in the same directory.\n",
      "     |      The events will go into a new events file.\n",
      "     |      \n",
      "     |      Does nothing if the summary writer was not closed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Supervisor(__builtin__.object)\n",
      "     |  A training helper that checkpoints models and computes summaries.\n",
      "     |  \n",
      "     |  The Supervisor is a small wrapper around a `Coordinator`, a `Saver`,\n",
      "     |  and a `SessionManager` that takes care of common needs of TensorFlow\n",
      "     |  training programs.\n",
      "     |  \n",
      "     |  #### Use for a single program\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  with tf.Graph().as_default():\n",
      "     |    ...add operations to the graph...\n",
      "     |    # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.\n",
      "     |    sv = Supervisor(logdir='/tmp/mydir')\n",
      "     |    # Get a TensorFlow session managed by the supervisor.\n",
      "     |    with sv.managed_session(FLAGS.master) as sess:\n",
      "     |      # Use the session to train the graph.\n",
      "     |      while not sv.should_stop():\n",
      "     |        sess.run(<my_train_op>)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Within the `with sv.managed_session()` block all variables in the graph have\n",
      "     |  been initialized.  In addition, a few services have been started to\n",
      "     |  checkpoint the model and add summaries to the event log.\n",
      "     |  \n",
      "     |  If the program crashes and is restarted, the managed session automatically\n",
      "     |  reinitialize variables from the most recent checkpoint.\n",
      "     |  \n",
      "     |  The supervisor is notified of any exception raised by one of the services.\n",
      "     |  After an exception is raised, `should_stop()` returns `True`.  In that case\n",
      "     |  the training loop should also stop.  This is why the training loop has to\n",
      "     |  check for `sv.should_stop()`.\n",
      "     |  \n",
      "     |  Exceptions that indicate that the training inputs have been exhausted,\n",
      "     |  `tf.errors.OutOfRangeError`, also cause `sv.should_stop()` to return `True`\n",
      "     |  but are not re-raised from the `with` block: they indicate a normal\n",
      "     |  termination.\n",
      "     |  \n",
      "     |  #### Use for multiple replicas\n",
      "     |  \n",
      "     |  To train with replicas you deploy the same program in a `Cluster`.\n",
      "     |  One of the tasks must be identified as the *chief*: the task that handles\n",
      "     |  initialization, checkpoints, summaries, and recovery.  The other tasks\n",
      "     |  depend on the *chief* for these services.\n",
      "     |  \n",
      "     |  The only change you have to do to the single program code is to indicate\n",
      "     |  if the program is running as the *chief*.\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  # Choose a task as the chief. This could be based on server_def.task_index,\n",
      "     |  # or job_def.name, or job_def.tasks. It's entirely up to the end user.\n",
      "     |  # But there can be only one *chief*.\n",
      "     |  is_chief = (server_def.task_index == 0)\n",
      "     |  server = tf.train.Server(server_def)\n",
      "     |  \n",
      "     |  with tf.Graph().as_default():\n",
      "     |    ...add operations to the graph...\n",
      "     |    # Create a Supervisor that uses log directory on a shared file system.\n",
      "     |    # Indicate if you are the 'chief'\n",
      "     |    sv = Supervisor(logdir='/shared_directory/...', is_chief=is_chief)\n",
      "     |    # Get a Session in a TensorFlow server on the cluster.\n",
      "     |    with sv.managed_session(server.target) as sess:\n",
      "     |      # Use the session to train the graph.\n",
      "     |      while not sv.should_stop():\n",
      "     |        sess.run(<my_train_op>)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  In the *chief* task, the `Supervisor` works exactly as in the first example\n",
      "     |  above.  In the other tasks `sv.managed_session()` waits for the Model to have\n",
      "     |  been intialized before returning a session to the training code.  The\n",
      "     |  non-chief tasks depend on the chief taks for initializing the model.\n",
      "     |  \n",
      "     |  If one of the tasks crashes and restarts, `managed_session()`\n",
      "     |  checks if the Model is initialized.  If yes, it just creates a session and\n",
      "     |  returns it to the training code that proceeds normally.  If the model needs\n",
      "     |  to be initialized, the chief task takes care of reinitializing it; the other\n",
      "     |  tasks just wait for the model to have been initialized.\n",
      "     |  \n",
      "     |  NOTE: This modified program still works fine as a single program.\n",
      "     |  The single program marks itself as the chief.\n",
      "     |  \n",
      "     |  #### What `master` string to use\n",
      "     |  \n",
      "     |  Whether you are running on your machine or in the cluster you can use the\n",
      "     |  following values for the --master flag:\n",
      "     |  \n",
      "     |  * Specifying `''` requests an in-process session that does not use RPC.\n",
      "     |  \n",
      "     |  * Specifying `'local'` requests a session that uses the RPC-based\n",
      "     |    \"Master interface\" to run TensorFlow programs. See\n",
      "     |    [`tf.train.Server.create_local_server()`](#Server.create_local_server) for\n",
      "     |    details.\n",
      "     |  \n",
      "     |  * Specifying `'grpc://hostname:port'` requests a session that uses\n",
      "     |    the RPC interface to a specific , and also allows the in-process\n",
      "     |    master to access remote tensorflow workers. Often, it is\n",
      "     |    appropriate to pass `server.target` (for some `tf.train.Server`\n",
      "     |    named `server).\n",
      "     |  \n",
      "     |  #### Advanced use\n",
      "     |  \n",
      "     |  ##### Launching additional services\n",
      "     |  \n",
      "     |  `managed_session()` launches the Checkpoint and Summary services (threads).\n",
      "     |  If you need more services to run you can simply launch them in the block\n",
      "     |  controlled by `managed_session()`.\n",
      "     |  \n",
      "     |  Example: Start a thread to print losses.  We want this thread to run\n",
      "     |  every 60 seconds, so we launch it with `sv.loop()`.\n",
      "     |  \n",
      "     |    ```python\n",
      "     |    ...\n",
      "     |    sv = Supervisor(logdir='/tmp/mydir')\n",
      "     |    with sv.managed_session(FLAGS.master) as sess:\n",
      "     |      sv.loop(60, print_loss, (sess))\n",
      "     |      while not sv.should_stop():\n",
      "     |        sess.run(my_train_op)\n",
      "     |    ```\n",
      "     |  \n",
      "     |  ##### Launching fewer services\n",
      "     |  \n",
      "     |  `managed_session()` launches the \"summary\" and \"checkpoint\" threads which use\n",
      "     |  either the optionally `summary_op` and `saver` passed to the constructor, or\n",
      "     |  default ones created automatically by the supervisor.  If you want to run\n",
      "     |  your own summary and checkpointing logic, disable these services by passing\n",
      "     |  `None` to the `summary_op` and `saver` parameters.\n",
      "     |  \n",
      "     |  Example: Create summaries manually every 100 steps in the chief.\n",
      "     |  \n",
      "     |    ```python\n",
      "     |    # Create a Supervisor with no automatic summaries.\n",
      "     |    sv = Supervisor(logdir='/tmp/mydir', is_chief=is_chief, summary_op=None)\n",
      "     |    # As summary_op was None, managed_session() does not start the\n",
      "     |    # summary thread.\n",
      "     |    with sv.managed_session(FLAGS.master) as sess:\n",
      "     |      for step in xrange(1000000):\n",
      "     |        if sv.should_stop():\n",
      "     |          break\n",
      "     |        if is_chief and step % 100 == 0:\n",
      "     |          # Create the summary every 100 chief steps.\n",
      "     |          sv.summary_computed(sess, sess.run(my_summary_op))\n",
      "     |        else:\n",
      "     |          # Train normally\n",
      "     |          sess.run(my_train_op)\n",
      "     |    ```\n",
      "     |  \n",
      "     |  ##### Custom model initialization\n",
      "     |  \n",
      "     |  `managed_session()` only supports initializing the model by running an\n",
      "     |  `init_op` or restoring from the latest checkpoint.  If you have special\n",
      "     |  initialization needs, see how to specify a `local_init_op` when creating the\n",
      "     |  supervisor.  You can also use the `SessionManager` directly to create a\n",
      "     |  session and check if it could be initialized automatically.\n",
      "     |  \n",
      "     |  @@__init__\n",
      "     |  @@managed_session\n",
      "     |  @@prepare_or_wait_for_session\n",
      "     |  @@start_standard_services\n",
      "     |  @@start_queue_runners\n",
      "     |  @@summary_computed\n",
      "     |  \n",
      "     |  @@stop\n",
      "     |  @@request_stop\n",
      "     |  @@should_stop\n",
      "     |  @@stop_on_exception\n",
      "     |  @@wait_for_stop\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  Loop = loop(self, timer_interval_secs, target, args=None, kwargs=None)\n",
      "     |  \n",
      "     |  PrepareSession = prepare_or_wait_for_session(self, master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)\n",
      "     |  \n",
      "     |  RequestStop = request_stop(self, ex=None)\n",
      "     |  \n",
      "     |  ShouldStop = should_stop(self)\n",
      "     |  \n",
      "     |  StartQueueRunners = start_queue_runners(self, sess, queue_runners=None)\n",
      "     |  \n",
      "     |  StartStandardServices = start_standard_services(self, sess)\n",
      "     |  \n",
      "     |  Stop = stop(self, threads=None, close_summary_writer=True)\n",
      "     |  \n",
      "     |  StopOnException = stop_on_exception(self)\n",
      "     |  \n",
      "     |  SummaryComputed = summary_computed(self, sess, summary, global_step=None)\n",
      "     |  \n",
      "     |  WaitForStop = wait_for_stop(self)\n",
      "     |  \n",
      "     |  __init__(self, graph=None, ready_op=0, is_chief=True, init_op=0, init_feed_dict=None, local_init_op=0, logdir=None, summary_op=0, saver=0, global_step=0, save_summaries_secs=120, save_model_secs=600, recovery_wait_secs=30, stop_grace_secs=120, checkpoint_basename='model.ckpt', session_manager=None, summary_writer=0, init_fn=None)\n",
      "     |      Create a `Supervisor`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        graph: A `Graph`.  The graph that the model will use.  Defaults to the\n",
      "     |          default `Graph`.  The supervisor may add operations to the graph before\n",
      "     |          creating a session, but the graph should not be modified by the caller\n",
      "     |          after passing it to the supervisor.\n",
      "     |        ready_op: 1-D string `Tensor`.  This tensor is evaluated by supervisors in\n",
      "     |          `prepare_or_wait_for_session()` to check if the model is ready to use.\n",
      "     |          The model is considered ready if it returns an empty array.  Defaults to\n",
      "     |          the tensor returned from `tf.report_uninitialized_variables()`  If\n",
      "     |          `None`, the model is not checked for readiness.\n",
      "     |        is_chief: If True, create a chief supervisor in charge of initializing\n",
      "     |          and restoring the model.  If False, create a supervisor that relies\n",
      "     |          on a chief supervisor for inits and restore.\n",
      "     |        init_op: `Operation`.  Used by chief supervisors to initialize the model\n",
      "     |          when it can not be recovered.  Defaults to an `Operation` that\n",
      "     |          initializes all variables.  If `None`, no initialization is done\n",
      "     |          automatically unless you pass a value for `init_fn`, see below.\n",
      "     |        init_feed_dict: A dictionary that maps `Tensor` objects to feed values.\n",
      "     |          This feed dictionary will be used when `init_op` is evaluated.\n",
      "     |        local_init_op: `Operation`. Used by all supervisors to run initializations\n",
      "     |          that should run for every new supervisor instance. By default these\n",
      "     |          are table initializers and initializers for local variables.\n",
      "     |          If `None`, no further per supervisor-instance initialization is\n",
      "     |          done automatically.\n",
      "     |        logdir: A string.  Optional path to a directory where to checkpoint the\n",
      "     |          model and log events for the visualizer.  Used by chief supervisors.\n",
      "     |          The directory will be created if it does not exist.\n",
      "     |        summary_op: An `Operation` that returns a Summary for the event logs.\n",
      "     |          Used by chief supervisors if a `logdir` was specified.  Defaults to the\n",
      "     |          operation returned from merge_all_summaries().  If `None`, summaries are\n",
      "     |          not computed automatically.\n",
      "     |        saver: A Saver object.  Used by chief supervisors if a `logdir` was\n",
      "     |          specified.  Defaults to the saved returned by Saver().\n",
      "     |          If `None`, the model is not saved automatically.\n",
      "     |        global_step: An integer Tensor of size 1 that counts steps.  The value\n",
      "     |          from 'global_step' is used in summaries and checkpoint filenames.\n",
      "     |          Default to the op named 'global_step' in the graph if it exists, is of\n",
      "     |          rank 1, size 1, and of type tf.int32 ot tf.int64.  If `None` the global\n",
      "     |          step is not recorded in summaries and checkpoint files.  Used by chief\n",
      "     |          supervisors if a `logdir` was specified.\n",
      "     |        save_summaries_secs: Number of seconds between the computation of\n",
      "     |          summaries for the event log.  Defaults to 120 seconds.  Pass 0 to\n",
      "     |          disable summaries.\n",
      "     |        save_model_secs: Number of seconds between the creation of model\n",
      "     |          checkpoints.  Defaults to 600 seconds.  Pass 0 to disable checkpoints.\n",
      "     |        recovery_wait_secs: Number of seconds between checks that the model\n",
      "     |          is ready.  Used by supervisors when waiting for a chief supervisor\n",
      "     |          to initialize or restore the model.  Defaults to 30 seconds.\n",
      "     |        stop_grace_secs: Grace period, in seconds, given to running threads to\n",
      "     |          stop when `stop()` is called.  Defaults to 120 seconds.\n",
      "     |        checkpoint_basename: The basename for checkpoint saving.\n",
      "     |        session_manager: `SessionManager`, which manages Session creation and\n",
      "     |          recovery. If it is `None`, a default `SessionManager` will be created\n",
      "     |          with the set of arguments passed in for backwards compatibility.\n",
      "     |        summary_writer: `SummaryWriter` to use or `USE_DEFAULT`.  Can be `None`\n",
      "     |          to indicate that no summaries should be written.\n",
      "     |        init_fn: Optional callable used to initialize the model. Called\n",
      "     |          after the optional `init_op` is called.  The callable must accept one\n",
      "     |          argument, the session being initialized.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A `Supervisor`.\n",
      "     |  \n",
      "     |  loop(self, timer_interval_secs, target, args=None, kwargs=None)\n",
      "     |      Start a LooperThread that calls a function periodically.\n",
      "     |      \n",
      "     |      If `timer_interval_secs` is None the thread calls `target(*args, **kwargs)`\n",
      "     |      repeatedly.  Otherwise it calls it every `timer_interval_secs`\n",
      "     |      seconds.  The thread terminates when a stop is requested.\n",
      "     |      \n",
      "     |      The started thread is added to the list of threads managed by the supervisor\n",
      "     |      so it does not need to be passed to the `stop()` method.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        timer_interval_secs: Number. Time boundaries at which to call `target`.\n",
      "     |        target: A callable object.\n",
      "     |        args: Optional arguments to pass to `target` when calling it.\n",
      "     |        kwargs: Optional keyword arguments to pass to `target` when calling it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The started thread.\n",
      "     |  \n",
      "     |  managed_session(*args, **kwds)\n",
      "     |      Returns a context manager for a managed session.\n",
      "     |      \n",
      "     |      This context manager creates and automatically recovers a session.  It\n",
      "     |      optionally starts the standard services that handle checkpoints and\n",
      "     |      summaries.  It monitors exceptions raised from the `with` block or from the\n",
      "     |      services and stops the supervisor as needed.\n",
      "     |      \n",
      "     |      The context manager is typically used as follows:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      def train():\n",
      "     |        sv = tf.train.Supervisor(...)\n",
      "     |        with sv.managed_session(<master>) as sess:\n",
      "     |          for step in xrange(..):\n",
      "     |            if sv.should_stop():\n",
      "     |              break\n",
      "     |            sess.run(<my training op>)\n",
      "     |            ...do other things needed at each training step...\n",
      "     |      ```\n",
      "     |      \n",
      "     |      An exception raised from the `with` block or one of the service threads is\n",
      "     |      raised again when the block exits.  This is done after stopping all threads\n",
      "     |      and closing the session.  For example, an `AbortedError` exception, raised\n",
      "     |      in case of preemption of one of the workers in a distributed model, is\n",
      "     |      raised again when the block exits.\n",
      "     |      \n",
      "     |      If you want to retry the training loop in case of preemption you can do it\n",
      "     |      as follows:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      def main(...):\n",
      "     |        while True\n",
      "     |          try:\n",
      "     |            train()\n",
      "     |          except tf.errors.Aborted:\n",
      "     |            pass\n",
      "     |      ```\n",
      "     |      \n",
      "     |      As a special case, exceptions used for control flow, such as\n",
      "     |      `OutOfRangeError` which reports that input queues are exhausted, are not\n",
      "     |      raised again from the `with` block: they indicate a clean termination of\n",
      "     |      the training loop and are considered normal termination.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        master: name of the TensorFlow master to use.  See the `tf.Session`\n",
      "     |          constructor for how this is interpreted.\n",
      "     |        config: Optional `ConfigProto` proto used to configure the session.\n",
      "     |          Passed as-is to create the session.\n",
      "     |        start_standard_services: Whether to start the standard services,\n",
      "     |          such as checkpoint, summary and step counter.\n",
      "     |        close_summary_writer: Whether to close the summary writer when\n",
      "     |          closing the session.  Defaults to True.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A context manager that yields a `Session` restored from the latest\n",
      "     |        checkpoint or initialized from scratch if not checkpoint exists.  The\n",
      "     |        session is closed when the `with` block exits.\n",
      "     |  \n",
      "     |  prepare_or_wait_for_session(self, master='', config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)\n",
      "     |      Make sure the model is ready to be used.\n",
      "     |      \n",
      "     |      Create a session on 'master', recovering or initializing the model as\n",
      "     |      needed, or wait for a session to be ready.  If running as the chief\n",
      "     |      and `start_standard_service` is set to True, also call the session\n",
      "     |      manager to start the standard services.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        master: name of the TensorFlow master to use.  See the `tf.Session`\n",
      "     |          constructor for how this is interpreted.\n",
      "     |        config: Optional ConfigProto proto used to configure the session,\n",
      "     |          which is passed as-is to create the session.\n",
      "     |        wait_for_checkpoint: Whether we should wait for the availability of a\n",
      "     |          checkpoint before creating Session. Defaults to False.\n",
      "     |        max_wait_secs: Maximum time to wait for the session to become available.\n",
      "     |        start_standard_services: Whether to start the standard services and the\n",
      "     |          queue runners.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A Session object that can be used to drive the model.\n",
      "     |  \n",
      "     |  request_stop(self, ex=None)\n",
      "     |      Request that the coordinator stop the threads.\n",
      "     |      \n",
      "     |      See `Coordinator.request_stop()`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        ex: Optional `Exception`, or Python `exc_info` tuple as returned by\n",
      "     |          `sys.exc_info()`.  If this is the first call to `request_stop()` the\n",
      "     |          corresponding exception is recorded and re-raised from `join()`.\n",
      "     |  \n",
      "     |  should_stop(self)\n",
      "     |      Check if the coordinator was told to stop.\n",
      "     |      \n",
      "     |      See `Coordinator.should_stop()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        True if the coordinator was told to stop, False otherwise.\n",
      "     |  \n",
      "     |  start_queue_runners(self, sess, queue_runners=None)\n",
      "     |      Start threads for `QueueRunners`.\n",
      "     |      \n",
      "     |      Note that the queue runners collected in the graph key `QUEUE_RUNNERS`\n",
      "     |      are already started automatically when you create a session with the\n",
      "     |      supervisor, so unless you have non-collected queue runners to start\n",
      "     |      you do not need to call this explicitely.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A `Session`.\n",
      "     |        queue_runners: A list of `QueueRunners`. If not specified, we'll use the\n",
      "     |          list of queue runners gathered in the graph under the key\n",
      "     |          `GraphKeys.QUEUE_RUNNERS`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        The list of threads started for the `QueueRunners`.\n",
      "     |  \n",
      "     |  start_standard_services(self, sess)\n",
      "     |      Start the standard services for 'sess'.\n",
      "     |      \n",
      "     |      This starts services in the background.  The services started depend\n",
      "     |      on the parameters to the constructor and may include:\n",
      "     |      \n",
      "     |        - A Summary thread computing summaries every save_summaries_secs.\n",
      "     |        - A Checkpoint thread saving the model every save_model_secs.\n",
      "     |        - A StepCounter thread measure step time.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A Session.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A list of threads that are running the standard services.  You can use\n",
      "     |        the Supervisor's Coordinator to join these threads with:\n",
      "     |          sv.coord.Join(<list of threads>)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        RuntimeError: If called with a non-chief Supervisor.\n",
      "     |        ValueError: If not `logdir` was passed to the constructor as the\n",
      "     |          services need a log directory.\n",
      "     |  \n",
      "     |  stop(self, threads=None, close_summary_writer=True)\n",
      "     |      Stop the services and the coordinator.\n",
      "     |      \n",
      "     |      This does not close the session.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        threads: Optional list of threads to join with the coordinator.  If\n",
      "     |          `None`, defaults to the threads running the standard services, the\n",
      "     |          threads started for `QueueRunners`, and the threads started by the\n",
      "     |          `loop()` method.  To wait on additional threads, pass the\n",
      "     |          list in this parameter.\n",
      "     |        close_summary_writer: Whether to close the `summary_writer`.  Defaults to\n",
      "     |          `True` if the summary writer was created by the supervisor, `False`\n",
      "     |          otherwise.\n",
      "     |  \n",
      "     |  stop_on_exception(self)\n",
      "     |      Context handler to stop the supervisor when an exception is raised.\n",
      "     |      \n",
      "     |      See `Coordinator.stop_on_exception()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A context handler.\n",
      "     |  \n",
      "     |  summary_computed(self, sess, summary, global_step=None)\n",
      "     |      Indicate that a summary was computed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        sess: A `Session` object.\n",
      "     |        summary: A Summary proto, or a string holding a serialized summary proto.\n",
      "     |        global_step: Int. global step this summary is associated with. If `None`,\n",
      "     |          it will try to fetch the current step.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        TypeError: if 'summary' is not a Summary proto or a string.\n",
      "     |        RuntimeError: if the Supervisor was created without a `logdir`.\n",
      "     |  \n",
      "     |  wait_for_stop(self)\n",
      "     |      Block waiting for the coordinator to stop.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  coord\n",
      "     |      Return the Coordinator used by the Supervisor.\n",
      "     |      \n",
      "     |      The Coordinator can be useful if you want to run multiple threads\n",
      "     |      during your training.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A Coordinator object.\n",
      "     |  \n",
      "     |  global_step\n",
      "     |      Return the global_step Tensor used by the supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An integer Tensor for the global_step.\n",
      "     |  \n",
      "     |  init_feed_dict\n",
      "     |      Return the feed dictionary used when evaluating the `init_op`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A feed dictionary or `None`.\n",
      "     |  \n",
      "     |  init_op\n",
      "     |      Return the Init Op used by the supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Op or `None`.\n",
      "     |  \n",
      "     |  is_chief\n",
      "     |      Return True if this is a chief supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A bool.\n",
      "     |  \n",
      "     |  ready_op\n",
      "     |      Return the Ready Op used by the supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        An Op or `None`.\n",
      "     |  \n",
      "     |  save_model_secs\n",
      "     |      Return the delay between checkpoints.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A timestamp.\n",
      "     |  \n",
      "     |  save_path\n",
      "     |      Return the save path used by the supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A string.\n",
      "     |  \n",
      "     |  save_summaries_secs\n",
      "     |      Return the delay between summary computations.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A timestamp.\n",
      "     |  \n",
      "     |  saver\n",
      "     |      Return the Saver used by the supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A Saver object.\n",
      "     |  \n",
      "     |  session_manager\n",
      "     |      Return the SessionManager used by the Supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A SessionManager object.\n",
      "     |  \n",
      "     |  summary_op\n",
      "     |      Return the Summary Tensor used by the chief supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A string Tensor for the summary or `None`.\n",
      "     |  \n",
      "     |  summary_writer\n",
      "     |      Return the SummaryWriter used by the chief supervisor.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A SummaryWriter.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  USE_DEFAULT = 0\n",
      "\n",
      "FUNCTIONS\n",
      "    add_queue_runner(qr, collection='queue_runners')\n",
      "        Adds a `QueueRunner` to a collection in the graph.\n",
      "        \n",
      "        When building a complex model that uses many queues it is often difficult to\n",
      "        gather all the queue runners that need to be run.  This convenience function\n",
      "        allows you to add a queue runner to a well known collection in the graph.\n",
      "        \n",
      "        The companion method `start_queue_runners()` can be used to start threads for\n",
      "        all the collected queue runners.\n",
      "        \n",
      "        Args:\n",
      "          qr: A `QueueRunner`.\n",
      "          collection: A `GraphKey` specifying the graph collection to add\n",
      "            the queue runner to.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n",
      "    \n",
      "    batch(tensors, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)\n",
      "        Creates batches of tensors in `tensors`.\n",
      "        \n",
      "        The argument `tensors` can be a list or a dictionary of tensors.\n",
      "        The value returned by the function will be of the same type\n",
      "        as `tensors`.\n",
      "        \n",
      "        This function is implemented using a queue. A `QueueRunner` for the\n",
      "        queue is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "        \n",
      "        If `enqueue_many` is `False`, `tensors` is assumed to represent a single\n",
      "        example.  An input tensor with shape `[x, y, z]` will be output as a tensor\n",
      "        with shape `[batch_size, x, y, z]`.\n",
      "        \n",
      "        If `enqueue_many` is `True`, `tensors` is assumed to represent a batch of\n",
      "        examples, where the first dimension is indexed by example, and all members of\n",
      "        `tensor_list` should have the same size in the first dimension.  If an input\n",
      "        tensor has shape `[*, x, y, z]`, the output will have shape `[batch_size, x,\n",
      "        y, z]`.  The `capacity` argument controls the how long the prefetching is\n",
      "        allowed to grow the queues.\n",
      "        \n",
      "        The returned operation is a dequeue operation and will throw\n",
      "        `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "        operation is feeding another input queue, its queue runner will catch\n",
      "        this exception, however, if this operation is used in your main thread\n",
      "        you are responsible for catching this yourself.\n",
      "        \n",
      "        *N.B.:* If `dynamic_pad` is `False`, you must ensure that either\n",
      "        (i) the `shapes` argument is passed, or (ii) all of the tensors in\n",
      "        `tensors` must have fully-defined shapes. `ValueError` will be\n",
      "        raised if neither of these conditions holds.\n",
      "        \n",
      "        If `dynamic_pad` is `True`, it is sufficient that the *rank* of the\n",
      "        tensors is known, but individual dimensions may have shape `None`.\n",
      "        In this case, for each enqueue the dimensions with value `None`\n",
      "        may have a variable length; upon dequeue, the output tensors will be padded\n",
      "        on the right to the maximum shape of the tensors in the current minibatch.\n",
      "        For numbers, this padding takes value 0.  For strings, this padding is\n",
      "        the empty string.  See `PaddingFIFOQueue` for more info.\n",
      "        \n",
      "        If `allow_smaller_final_batch` is `True`, a smaller batch value than\n",
      "        `batch_size` is returned when the queue is closed and there are not enough\n",
      "        elements to fill the batch, otherwise the pending elements are discarded.\n",
      "        In addition, all output tensors' static shapes, as accessed via the\n",
      "        `get_shape` method will have a first `Dimension` value of `None`, and\n",
      "        operations that depend on fixed batch_size would fail.\n",
      "        \n",
      "        Args:\n",
      "          tensors: The list or dictionary of tensors to enqueue.\n",
      "          batch_size: The new batch size pulled from the queue.\n",
      "          num_threads: The number of threads enqueuing `tensor_list`.\n",
      "          capacity: An integer. The maximum number of elements in the queue.\n",
      "          enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "          shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "            inferred shapes for `tensor_list`.\n",
      "          dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n",
      "            The given dimensions are padded upon dequeue so that tensors within a\n",
      "            batch have the same shapes.\n",
      "          allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n",
      "            batch to be smaller if there are insufficient items left in the queue.\n",
      "          shared_name: (Optional). If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: (Optional) A name for the operations.\n",
      "        \n",
      "        Returns:\n",
      "          A list or dictionary of tensors with the same types as `tensors`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the `shapes` are not specified, and cannot be\n",
      "            inferred from the elements of `tensors`.\n",
      "    \n",
      "    batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False, shapes=None, dynamic_pad=False, allow_smaller_final_batch=False, shared_name=None, name=None)\n",
      "        Runs a list of tensors to fill a queue to create batches of examples.\n",
      "        \n",
      "        The `tensors_list` argument is a list of tuples of tensors, or a list of\n",
      "        dictionaries of tensors.  Each element in the list is treated similarily\n",
      "        to the `tensors` argument of `tf.train.batch()`.\n",
      "        \n",
      "        Enqueues a different list of tensors in different threads.\n",
      "        Implemented using a queue -- a `QueueRunner` for the queue\n",
      "        is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "        \n",
      "        `len(tensors_list)` threads will be started,\n",
      "        with thread `i` enqueuing the tensors from\n",
      "        `tensors_list[i]`. `tensors_list[i1][j]` must match\n",
      "        `tensors_list[i2][j]` in type and shape, except in the first\n",
      "        dimension if `enqueue_many` is true.\n",
      "        \n",
      "        If `enqueue_many` is `False`, each `tensors_list[i]` is assumed\n",
      "        to represent a single example. An input tensor `x` will be output as a\n",
      "        tensor with shape `[batch_size] + x.shape`.\n",
      "        \n",
      "        If `enqueue_many` is `True`, `tensors_list[i]` is assumed to\n",
      "        represent a batch of examples, where the first dimension is indexed\n",
      "        by example, and all members of `tensors_list[i]` should have the\n",
      "        same size in the first dimension.  The slices of any input tensor\n",
      "        `x` are treated as examples, and the output tensors will have shape\n",
      "        `[batch_size] + x.shape[1:]`.\n",
      "        \n",
      "        The `capacity` argument controls the how long the prefetching is allowed to\n",
      "        grow the queues.\n",
      "        \n",
      "        The returned operation is a dequeue operation and will throw\n",
      "        `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "        operation is feeding another input queue, its queue runner will catch\n",
      "        this exception, however, if this operation is used in your main thread\n",
      "        you are responsible for catching this yourself.\n",
      "        \n",
      "        *N.B.:* If `dynamic_pad` is `False`, you must ensure that either\n",
      "        (i) the `shapes` argument is passed, or (ii) all of the tensors in\n",
      "        `tensors_list` must have fully-defined shapes. `ValueError` will be\n",
      "        raised if neither of these conditions holds.\n",
      "        \n",
      "        If `dynamic_pad` is `True`, it is sufficient that the *rank* of the\n",
      "        tensors is known, but individual dimensions may have value `None`.\n",
      "        In this case, for each enqueue the dimensions with value `None`\n",
      "        may have a variable length; upon dequeue, the output tensors will be padded\n",
      "        on the right to the maximum shape of the tensors in the current minibatch.\n",
      "        For numbers, this padding takes value 0.  For strings, this padding is\n",
      "        the empty string.  See `PaddingFIFOQueue` for more info.\n",
      "        \n",
      "        If `allow_smaller_final_batch` is `True`, a smaller batch value than\n",
      "        `batch_size` is returned when the queue is closed and there are not enough\n",
      "        elements to fill the batch, otherwise the pending elements are discarded.\n",
      "        In addition, all output tensors' static shapes, as accessed via the\n",
      "        `get_shape` method will have a first `Dimension` value of `None`, and\n",
      "        operations that depend on fixed batch_size would fail.\n",
      "        \n",
      "        Args:\n",
      "          tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n",
      "          batch_size: An integer. The new batch size pulled from the queue.\n",
      "          capacity: An integer. The maximum number of elements in the queue.\n",
      "          enqueue_many: Whether each tensor in `tensor_list_list` is a single\n",
      "            example.\n",
      "          shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "            inferred shapes for `tensor_list_list[i]`.\n",
      "          dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n",
      "            The given dimensions are padded upon dequeue so that tensors within a\n",
      "            batch have the same shapes.\n",
      "          allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n",
      "            batch to be smaller if there are insufficient items left in the queue.\n",
      "          shared_name: (Optional) If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: (Optional) A name for the operations.\n",
      "        \n",
      "        Returns:\n",
      "          A list or dictionary of tensors with the same number and types as\n",
      "          `tensors_list[i]`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the `shapes` are not specified, and cannot be\n",
      "            inferred from the elements of `tensor_list_list`.\n",
      "    \n",
      "    do_quantize_training_on_graphdef(input_graph, num_bits)\n",
      "    \n",
      "    exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)\n",
      "        Applies exponential decay to the learning rate.\n",
      "        \n",
      "        When training a model, it is often recommended to lower the learning rate as\n",
      "        the training progresses.  This function applies an exponential decay function\n",
      "        to a provided initial learning rate.  It requires a `global_step` value to\n",
      "        compute the decayed learning rate.  You can just pass a TensorFlow variable\n",
      "        that you increment at each training step.\n",
      "        \n",
      "        The function returns the decayed learning rate.  It is computed as:\n",
      "        \n",
      "        ```python\n",
      "        decayed_learning_rate = learning_rate *\n",
      "                                decay_rate ^ (global_step / decay_steps)\n",
      "        ```\n",
      "        \n",
      "        If the argument `staircase` is `True`, then `global_step / decay_steps` is an\n",
      "        integer division and the decayed learning rate follows a staircase function.\n",
      "        \n",
      "        Example: decay every 100000 steps with a base of 0.96:\n",
      "        \n",
      "        ```python\n",
      "        ...\n",
      "        global_step = tf.Variable(0, trainable=False)\n",
      "        starter_learning_rate = 0.1\n",
      "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
      "                                                   100000, 0.96, staircase=True)\n",
      "        # Passing global_step to minimize() will increment it at each step.\n",
      "        learning_step = (\n",
      "            tf.train.GradientDescentOptimizer(learning_rate)\n",
      "            .minimize(...my loss..., global_step=global_step)\n",
      "        )\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
      "            Python number.  The initial learning rate.\n",
      "          global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
      "            Global step to use for the decay computation.  Must not be negative.\n",
      "          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
      "            Must be positive.  See the decay computation above.\n",
      "          decay_rate: A scalar `float32` or `float64` `Tensor` or a\n",
      "            Python number.  The decay rate.\n",
      "          staircase: Boolean.  It `True` decay the learning rate at discrete intervals\n",
      "          name: String.  Optional name of the operation.  Defaults to \n",
      "            'ExponentialDecay'\n",
      "        \n",
      "        Returns:\n",
      "          A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n",
      "          learning rate.\n",
      "    \n",
      "    export_meta_graph(filename=None, meta_info_def=None, graph_def=None, saver_def=None, collection_list=None, as_text=False)\n",
      "        Returns `MetaGraphDef` proto. Optionally writes it to filename.\n",
      "        \n",
      "        This function exports the graph, saver, and collection objects into\n",
      "        `MetaGraphDef` protocol buffer with the intension of it being imported\n",
      "        at a later time or location to restart training, run inference, or be\n",
      "        a subgraph.\n",
      "        \n",
      "        Args:\n",
      "          filename: Optional filename including the path for writing the\n",
      "            generated `MetaGraphDef` protocol buffer.\n",
      "          meta_info_def: `MetaInfoDef` protocol buffer.\n",
      "          graph_def: `GraphDef` protocol buffer.\n",
      "          saver_def: `SaverDef` protocol buffer.\n",
      "          collection_list: List of string keys to collect.\n",
      "          as_text: If `True`, writes the `MetaGraphDef` as an ASCII proto.\n",
      "        \n",
      "        Returns:\n",
      "          A `MetaGraphDef` proto.\n",
      "    \n",
      "    generate_checkpoint_state_proto(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None)\n",
      "        Generates a checkpoint state proto.\n",
      "        \n",
      "        Args:\n",
      "          save_dir: Directory where the model was saved.\n",
      "          model_checkpoint_path: The checkpoint file.\n",
      "          all_model_checkpoint_paths: List of strings.  Paths to all not-yet-deleted\n",
      "            checkpoints, sorted from oldest to newest.  If this is a non-empty list,\n",
      "            the last element must be equal to model_checkpoint_path.  These paths\n",
      "            are also saved in the CheckpointState proto.\n",
      "        \n",
      "        Returns:\n",
      "          CheckpointState proto with model_checkpoint_path and\n",
      "          all_model_checkpoint_paths updated to either absolute paths or\n",
      "          relative paths to the current save_dir.\n",
      "    \n",
      "    get_checkpoint_state(checkpoint_dir, latest_filename=None)\n",
      "        Returns CheckpointState proto from the \"checkpoint\" file.\n",
      "        \n",
      "        If the \"checkpoint\" file contains a valid CheckpointState\n",
      "        proto, returns it.\n",
      "        \n",
      "        Args:\n",
      "          checkpoint_dir: The directory of checkpoints.\n",
      "          latest_filename: Optional name of the checkpoint file.  Default to\n",
      "            'checkpoint'.\n",
      "        \n",
      "        Returns:\n",
      "          A CheckpointState if the state was available, None\n",
      "          otherwise.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: if the checkpoint read doesn't have model_checkpoint_path set.\n",
      "    \n",
      "    global_step(sess, global_step_tensor)\n",
      "        Small helper to get the global step.\n",
      "        \n",
      "        ```python\n",
      "        # Creates a variable to hold the global_step.\n",
      "        global_step_tensor = tf.Variable(10, trainable=False, name='global_step')\n",
      "        # Creates a session.\n",
      "        sess = tf.Session()\n",
      "        # Initializes the variable.\n",
      "        sess.run(global_step_tensor.initializer)\n",
      "        print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\n",
      "        \n",
      "        global_step: 10\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          sess: A TensorFlow `Session` object.\n",
      "          global_step_tensor:  `Tensor` or the `name` of the operation that contains\n",
      "            the global step.\n",
      "        \n",
      "        Returns:\n",
      "          The global step value.\n",
      "    \n",
      "    import_meta_graph(meta_graph_or_file)\n",
      "        Recreates a Graph saved in a `MetaGraphDef` proto.\n",
      "        \n",
      "        This function takes a `MetaGraphDef` protocol buffer as input. If\n",
      "        the argument is a file containing a `MetaGraphDef` protocol buffer ,\n",
      "        it constructs a protocol buffer from the file content. The function\n",
      "        then adds all the nodes from the `graph_def` field to the\n",
      "        current graph, recreates all the collections, and returns a saver\n",
      "        constructed from the `saver_def` field.\n",
      "        \n",
      "        In combination with `export_meta_graph()`, this function can be used to\n",
      "        \n",
      "        * Serialize a graph along with other Python objects such as `QueueRunner`,\n",
      "          `Variable` into a `MetaGraphDef`.\n",
      "        \n",
      "        * Restart training from a saved graph and checkpoints.\n",
      "        \n",
      "        * Run inference from a saved graph and checkpoints.\n",
      "        \n",
      "        ```Python\n",
      "        ...\n",
      "        # Create a saver.\n",
      "        saver = tf.train.Saver(...variables...)\n",
      "        # Remember the training_op we want to run by adding it to a collection.\n",
      "        tf.add_to_collection('train_op', train_op)\n",
      "        sess = tf.Session()\n",
      "        for step in xrange(1000000):\n",
      "            sess.run(train_op)\n",
      "            if step % 1000 == 0:\n",
      "                # Saves checkpoint, which by default also exports a meta_graph\n",
      "                # named 'my-model-global_step.meta'.\n",
      "                saver.save(sess, 'my-model', global_step=step)\n",
      "        ```\n",
      "        \n",
      "        Later we can continue training from this saved `meta_graph` without building\n",
      "        the model from scratch.\n",
      "        \n",
      "        ```Python\n",
      "        with tf.Session() as sess:\n",
      "          new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n",
      "          new_saver.restore(sess, 'my-save-dir/my-model-10000')\n",
      "          # tf.get_collection() returns a list. In this example we only want the\n",
      "          # first one.\n",
      "          train_op = tf.get_collection('train_op')[0]\n",
      "          for step in xrange(1000000):\n",
      "            sess.run(train_op)\n",
      "        ```\n",
      "        \n",
      "        NOTE: Restarting training from saved `meta_graph` only works if the\n",
      "        device assignments have not changed.\n",
      "        \n",
      "        Args:\n",
      "          meta_graph_or_file: `MetaGraphDef` protocol buffer or filename (including\n",
      "            the path) containing a `MetaGraphDef`.\n",
      "        \n",
      "        Returns:\n",
      "          A saver constructed from `saver_def` in `MetaGraphDef` or None.\n",
      "        \n",
      "          A None value is returned if no variables exist in the `MetaGraphDef`\n",
      "          (i.e., there are no variables to restore).\n",
      "    \n",
      "    input_producer(input_tensor, element_shape=None, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, summary_name=None, name=None)\n",
      "        Output the rows of `input_tensor` to a queue for an input pipeline.\n",
      "        \n",
      "        Args:\n",
      "          input_tensor: A tensor with the rows to produce. Must be at least\n",
      "            one-dimensional. Must either have a fully-defined shape, or\n",
      "            `element_shape` must be defined.\n",
      "          element_shape: (Optional.) A `TensorShape` representing the shape of a\n",
      "            row of `input_tensor`, if it cannot be inferred.\n",
      "          num_epochs: (Optional.) An integer. If specified `input_producer` produces\n",
      "            each row of `input_tensor` `num_epochs` times before generating an\n",
      "            `OutOfRange` error. If not specified, `input_producer` can cycle through\n",
      "            the rows of `input_tensor` an unlimited number of times.\n",
      "          shuffle: (Optional.) A boolean. If true, the rows are randomly shuffled\n",
      "            within each eopch.\n",
      "          seed: (Optional.) An integer. The seed to use if `shuffle` is true.\n",
      "          capacity: (Optional.) The capacity of the queue to be used for buffering\n",
      "            the input.\n",
      "          shared_name: (Optional.) If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          summary_name: (Optional.) If set, a scalar summary for the current queue\n",
      "            size will be generated, using this name as part of the tag.\n",
      "          name: (Optional.) A name for queue.\n",
      "        \n",
      "        Returns:\n",
      "          A queue with the output rows.  A `QueueRunner` for the queue is\n",
      "          added to the current `QUEUE_RUNNER` collection of the current\n",
      "          graph.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the shape of the input cannot be inferred from the arguments.\n",
      "    \n",
      "    latest_checkpoint(checkpoint_dir, latest_filename=None)\n",
      "        Finds the filename of latest saved checkpoint file.\n",
      "        \n",
      "        Args:\n",
      "          checkpoint_dir: Directory where the variables were saved.\n",
      "          latest_filename: Optional name for the protocol buffer file that\n",
      "            contains the list of most recent checkpoint filenames.\n",
      "            See the corresponding argument to `Saver.save()`.\n",
      "        \n",
      "        Returns:\n",
      "          The full path to the latest checkpoint or `None` if no checkpoint was found.\n",
      "    \n",
      "    limit_epochs(tensor, num_epochs=None, name=None)\n",
      "        Returns tensor `num_epochs` times and then raises an `OutOfRange` error.\n",
      "        \n",
      "        Args:\n",
      "          tensor: Any `Tensor`.\n",
      "          num_epochs: A positive integer (optional).  If specified, limits the number\n",
      "            of steps the output tensor may be evaluated.\n",
      "          name: A name for the operations (optional).\n",
      "        \n",
      "        Returns:\n",
      "          tensor or `OutOfRange`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: if `num_epochs` is invalid.\n",
      "    \n",
      "    match_filenames_once(pattern, name=None)\n",
      "        Save the list of files matching pattern, so it is only computed once.\n",
      "        \n",
      "        Args:\n",
      "          pattern: A file pattern (glob).\n",
      "          name: A name for the operations (optional).\n",
      "        \n",
      "        Returns:\n",
      "          A variable that is initialized to the list of files matching pattern.\n",
      "    \n",
      "    range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)\n",
      "        Produces the integers from 0 to limit-1 in a queue.\n",
      "        \n",
      "        Args:\n",
      "          limit: An int32 scalar tensor.\n",
      "          num_epochs: An integer (optional). If specified, `range_input_producer`\n",
      "            produces each integer `num_epochs` times before generating an\n",
      "            OutOfRange error. If not specified, `range_input_producer` can cycle\n",
      "            through the integers an unlimited number of times.\n",
      "          shuffle: Boolean. If true, the integers are randomly shuffled within each\n",
      "            epoch.\n",
      "          seed: An integer (optional). Seed used if shuffle == True.\n",
      "          capacity: An integer. Sets the queue capacity.\n",
      "          shared_name: (optional). If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: A name for the operations (optional).\n",
      "        \n",
      "        Returns:\n",
      "          A Queue with the output integers.  A `QueueRunner` for the Queue\n",
      "          is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "    \n",
      "    replica_device_setter(ps_tasks=0, ps_device='/job:ps', worker_device='/job:worker', merge_devices=True, cluster=None, ps_ops=None)\n",
      "        Return a `device function` to use when building a Graph for replicas.\n",
      "        \n",
      "        Device Functions are used in `with tf.device(device_function):` statement to\n",
      "        automatically assign devices to `Operation` objects as they are constructed,\n",
      "        Device constraints are added from the inner-most context first, working\n",
      "        outwards. The merging behavior adds constraints to fields that are yet unset\n",
      "        by a more inner context. Currently the fields are (job, task, cpu/gpu).\n",
      "        \n",
      "        If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.\n",
      "        \n",
      "        For example,\n",
      "        \n",
      "        ```python\n",
      "        # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker\n",
      "        # jobs on hosts worker0, worker1 and worker2.\n",
      "        cluster_spec = {\n",
      "            \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n",
      "            \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\n",
      "        with tf.device(tf.replica_device_setter(cluster=cluster_spec)):\n",
      "          # Build your graph\n",
      "          v1 = tf.Variable(...)  # assigned to /job:ps/task:0\n",
      "          v2 = tf.Variable(...)  # assigned to /job:ps/task:1\n",
      "          v3 = tf.Variable(...)  # assigned to /job:ps/task:0\n",
      "        # Run compute\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          ps_tasks: Number of tasks in the `ps` job.\n",
      "          ps_device: String.  Device of the `ps` job.  If empty no `ps` job is used.\n",
      "            Defaults to `ps`.\n",
      "          worker_device: String.  Device of the `worker` job.  If empty no `worker`\n",
      "            job is used.\n",
      "          merge_devices: `Boolean`. If `True`, merges or only sets a device if the\n",
      "            device constraint is completely unset. merges device specification rather\n",
      "            than overriding them.\n",
      "          cluster: `ClusterDef` proto or `ClusterSpec`.\n",
      "          ps_ops: List of `Operation` objects that need to be placed on `ps` devices.\n",
      "        \n",
      "        Returns:\n",
      "          A function to pass to `tf.device()`.\n",
      "        \n",
      "        Raises:\n",
      "          TypeError if `cluster` is not a dictionary or `ClusterDef` protocol buffer.\n",
      "    \n",
      "    shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)\n",
      "        Creates batches by randomly shuffling tensors.\n",
      "        \n",
      "        This function adds the following to the current `Graph`:\n",
      "        \n",
      "        * A shuffling queue into which tensors from `tensors` are enqueued.\n",
      "        * A `dequeue_many` operation to create batches from the queue.\n",
      "        * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n",
      "          from `tensors`.\n",
      "        \n",
      "        If `enqueue_many` is `False`, `tensors` is assumed to represent a\n",
      "        single example.  An input tensor with shape `[x, y, z]` will be output\n",
      "        as a tensor with shape `[batch_size, x, y, z]`.\n",
      "        \n",
      "        If `enqueue_many` is `True`, `tensors` is assumed to represent a\n",
      "        batch of examples, where the first dimension is indexed by example,\n",
      "        and all members of `tensors` should have the same size in the\n",
      "        first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n",
      "        output will have shape `[batch_size, x, y, z]`.\n",
      "        \n",
      "        The `capacity` argument controls the how long the prefetching is allowed to\n",
      "        grow the queues.\n",
      "        \n",
      "        The returned operation is a dequeue operation and will throw\n",
      "        `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "        operation is feeding another input queue, its queue runner will catch\n",
      "        this exception, however, if this operation is used in your main thread\n",
      "        you are responsible for catching this yourself.\n",
      "        \n",
      "        For example:\n",
      "        \n",
      "        ```python\n",
      "        # Creates batches of 32 images and 32 labels.\n",
      "        image_batch, label_batch = tf.train.shuffle_batch(\n",
      "              [single_image, single_label],\n",
      "              batch_size=32,\n",
      "              num_threads=4,\n",
      "              capacity=50000,\n",
      "              min_after_dequeue=10000)\n",
      "        ```\n",
      "        \n",
      "        *N.B.:* You must ensure that either (i) the `shapes` argument is\n",
      "        passed, or (ii) all of the tensors in `tensors` must have\n",
      "        fully-defined shapes. `ValueError` will be raised if neither of\n",
      "        these conditions holds.\n",
      "        \n",
      "        If `allow_smaller_final_batch` is `True`, a smaller batch value than\n",
      "        `batch_size` is returned when the queue is closed and there are not enough\n",
      "        elements to fill the batch, otherwise the pending elements are discarded.\n",
      "        In addition, all output tensors' static shapes, as accessed via the\n",
      "        `get_shape` method will have a first `Dimension` value of `None`, and\n",
      "        operations that depend on fixed batch_size would fail.\n",
      "        \n",
      "        Args:\n",
      "          tensors: The list or dictionary of tensors to enqueue.\n",
      "          batch_size: The new batch size pulled from the queue.\n",
      "          capacity: An integer. The maximum number of elements in the queue.\n",
      "          min_after_dequeue: Minimum number elements in the queue after a\n",
      "            dequeue, used to ensure a level of mixing of elements.\n",
      "          num_threads: The number of threads enqueuing `tensor_list`.\n",
      "          seed: Seed for the random shuffling within the queue.\n",
      "          enqueue_many: Whether each tensor in `tensor_list` is a single example.\n",
      "          shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "            inferred shapes for `tensor_list`.\n",
      "          allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n",
      "            batch to be smaller if there are insufficient items left in the queue.\n",
      "          shared_name: (Optional) If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: (Optional) A name for the operations.\n",
      "        \n",
      "        Returns:\n",
      "          A list or dictionary of tensors with the types as `tensors`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the `shapes` are not specified, and cannot be\n",
      "            inferred from the elements of `tensors`.\n",
      "    \n",
      "    shuffle_batch_join(tensors_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, allow_smaller_final_batch=False, shared_name=None, name=None)\n",
      "        Create batches by randomly shuffling tensors.\n",
      "        \n",
      "        The `tensors_list` argument is a list of tuples of tensors, or a list of\n",
      "        dictionaries of tensors.  Each element in the list is treated similarily\n",
      "        to the `tensors` argument of `tf.train.shuffle_batch()`.\n",
      "        \n",
      "        This version enqueues a different list of tensors in different threads.\n",
      "        It adds the following to the current `Graph`:\n",
      "        \n",
      "        * A shuffling queue into which tensors from `tensors_list` are enqueued.\n",
      "        * A `dequeue_many` operation to create batches from the queue.\n",
      "        * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n",
      "          from `tensors_list`.\n",
      "        \n",
      "        `len(tensors_list)` threads will be started, with thread `i` enqueuing\n",
      "        the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match\n",
      "        `tensors_list[i2][j]` in type and shape, except in the first dimension if\n",
      "        `enqueue_many` is true.\n",
      "        \n",
      "        If `enqueue_many` is `False`, each `tensors_list[i]` is assumed\n",
      "        to represent a single example.  An input tensor with shape `[x, y, z]`\n",
      "        will be output as a tensor with shape `[batch_size, x, y, z]`.\n",
      "        \n",
      "        If `enqueue_many` is `True`, `tensors_list[i]` is assumed to\n",
      "        represent a batch of examples, where the first dimension is indexed\n",
      "        by example, and all members of `tensors_list[i]` should have the\n",
      "        same size in the first dimension.  If an input tensor has shape `[*, x,\n",
      "        y, z]`, the output will have shape `[batch_size, x, y, z]`.\n",
      "        \n",
      "        The `capacity` argument controls the how long the prefetching is allowed to\n",
      "        grow the queues.\n",
      "        \n",
      "        The returned operation is a dequeue operation and will throw\n",
      "        `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n",
      "        operation is feeding another input queue, its queue runner will catch\n",
      "        this exception, however, if this operation is used in your main thread\n",
      "        you are responsible for catching this yourself.\n",
      "        \n",
      "        If `allow_smaller_final_batch` is `True`, a smaller batch value than\n",
      "        `batch_size` is returned when the queue is closed and there are not enough\n",
      "        elements to fill the batch, otherwise the pending elements are discarded.\n",
      "        In addition, all output tensors' static shapes, as accessed via the\n",
      "        `get_shape` method will have a first `Dimension` value of `None`, and\n",
      "        operations that depend on fixed batch_size would fail.\n",
      "        \n",
      "        Args:\n",
      "          tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n",
      "          batch_size: An integer. The new batch size pulled from the queue.\n",
      "          capacity: An integer. The maximum number of elements in the queue.\n",
      "          min_after_dequeue: Minimum number elements in the queue after a\n",
      "            dequeue, used to ensure a level of mixing of elements.\n",
      "          seed: Seed for the random shuffling within the queue.\n",
      "          enqueue_many: Whether each tensor in `tensor_list_list` is a single\n",
      "            example.\n",
      "          shapes: (Optional) The shapes for each example.  Defaults to the\n",
      "            inferred shapes for `tensors_list[i]`.\n",
      "          allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n",
      "            batch to be smaller if there are insufficient items left in the queue.\n",
      "          shared_name: (optional). If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: (Optional) A name for the operations.\n",
      "        \n",
      "        Returns:\n",
      "          A list or dictionary of tensors with the same number and types as\n",
      "          `tensors_list[i]`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the `shapes` are not specified, and cannot be\n",
      "            inferred from the elements of `tensors_list`.\n",
      "    \n",
      "    slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)\n",
      "        Produces a slice of each `Tensor` in `tensor_list`.\n",
      "        \n",
      "        Implemented using a Queue -- a `QueueRunner` for the Queue\n",
      "        is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "        \n",
      "        Args:\n",
      "          tensor_list: A list of `Tensor` objects. Every `Tensor` in\n",
      "            `tensor_list` must have the same size in the first dimension.\n",
      "          num_epochs: An integer (optional). If specified, `slice_input_producer`\n",
      "            produces each slice `num_epochs` times before generating\n",
      "            an `OutOfRange` error. If not specified, `slice_input_producer` can cycle\n",
      "            through the slices an unlimited number of times.\n",
      "          shuffle: Boolean. If true, the integers are randomly shuffled within each\n",
      "            epoch.\n",
      "          seed: An integer (optional). Seed used if shuffle == True.\n",
      "          capacity: An integer. Sets the queue capacity.\n",
      "          shared_name: (optional). If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: A name for the operations (optional).\n",
      "        \n",
      "        Returns:\n",
      "          A list of tensors, one for each element of `tensor_list`.  If the tensor\n",
      "          in `tensor_list` has shape `[N, a, b, .., z]`, then the corresponding output\n",
      "          tensor will have shape `[a, b, ..., z]`.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: if `slice_input_producer` produces nothing from `tensor_list`.\n",
      "    \n",
      "    start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')\n",
      "        Starts all queue runners collected in the graph.\n",
      "        \n",
      "        This is a companion method to `add_queue_runner()`.  It just starts\n",
      "        threads for all queue runners collected in the graph.  It returns\n",
      "        the list of all threads.\n",
      "        \n",
      "        Args:\n",
      "          sess: `Session` used to run the queue ops.  Defaults to the\n",
      "            default session.\n",
      "          coord: Optional `Coordinator` for coordinating the started threads.\n",
      "          daemon: Whether the threads should be marked as `daemons`, meaning\n",
      "            they don't block program exit.\n",
      "          start: Set to `False` to only create the threads, not start them.\n",
      "          collection: A `GraphKey` specifying the graph collection to\n",
      "            get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n",
      "        \n",
      "        Returns:\n",
      "          A list of threads.\n",
      "    \n",
      "    string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None)\n",
      "        Output strings (e.g. filenames) to a queue for an input pipeline.\n",
      "        \n",
      "        Args:\n",
      "          string_tensor: A 1-D string tensor with the strings to produce.\n",
      "          num_epochs: An integer (optional). If specified, `string_input_producer`\n",
      "            produces each string from `string_tensor` `num_epochs` times before\n",
      "            generating an `OutOfRange` error. If not specified,\n",
      "            `string_input_producer` can cycle through the strings in `string_tensor`\n",
      "            an unlimited number of times.\n",
      "          shuffle: Boolean. If true, the strings are randomly shuffled within each\n",
      "            epoch.\n",
      "          seed: An integer (optional). Seed used if shuffle == True.\n",
      "          capacity: An integer. Sets the queue capacity.\n",
      "          shared_name: (optional). If set, this queue will be shared under the given\n",
      "            name across multiple sessions.\n",
      "          name: A name for the operations (optional).\n",
      "        \n",
      "        Returns:\n",
      "          A queue with the output strings.  A `QueueRunner` for the Queue\n",
      "          is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If the string_tensor is a null Python list.  At runtime,\n",
      "          will fail with an assertion if string_tensor becomes a null tensor.\n",
      "    \n",
      "    summary_iterator(path)\n",
      "        An iterator for reading `Event` protocol buffers from an event file.\n",
      "        \n",
      "        You can use this function to read events written to an event file. It returns\n",
      "        a Python iterator that yields `Event` protocol buffers.\n",
      "        \n",
      "        Example: Print the contents of an events file.\n",
      "        \n",
      "        ```python\n",
      "        for e in tf.train.summary_iterator(path to events file):\n",
      "            print(e)\n",
      "        ```\n",
      "        \n",
      "        Example: Print selected summary values.\n",
      "        \n",
      "        ```python\n",
      "        # This example supposes that the events file contains summaries with a\n",
      "        # summary value tag 'loss'.  These could have been added by calling\n",
      "        # `add_summary()`, passing the output of a scalar summary op created with\n",
      "        # with: `tf.scalar_summary(['loss'], loss_tensor)`.\n",
      "        for e in tf.train.summary_iterator(path to events file):\n",
      "            for v in e.summary.value:\n",
      "                if v.tag == 'loss':\n",
      "                    print(v.simple_value)\n",
      "        ```\n",
      "        \n",
      "        See the protocol buffer definitions of\n",
      "        [Event](https://www.tensorflow.org/code/tensorflow/core/util/event.proto)\n",
      "        and\n",
      "        [Summary](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\n",
      "        for more information about their attributes.\n",
      "        \n",
      "        Args:\n",
      "          path: The path to an event file created by a `SummaryWriter`.\n",
      "        \n",
      "        Yields:\n",
      "          `Event` protocol buffers.\n",
      "    \n",
      "    update_checkpoint_state(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None, latest_filename=None)\n",
      "        Updates the content of the 'checkpoint' file.\n",
      "        \n",
      "        This updates the checkpoint file containing a CheckpointState\n",
      "        proto.\n",
      "        \n",
      "        Args:\n",
      "          save_dir: Directory where the model was saved.\n",
      "          model_checkpoint_path: The checkpoint file.\n",
      "          all_model_checkpoint_paths: List of strings.  Paths to all not-yet-deleted\n",
      "            checkpoints, sorted from oldest to newest.  If this is a non-empty list,\n",
      "            the last element must be equal to model_checkpoint_path.  These paths\n",
      "            are also saved in the CheckpointState proto.\n",
      "          latest_filename: Optional name of the checkpoint file.  Default to\n",
      "            'checkpoint'.\n",
      "        \n",
      "        Raises:\n",
      "          RuntimeError: If the save paths conflict.\n",
      "    \n",
      "    write_graph(graph_def, logdir, name, as_text=True)\n",
      "        Writes a graph proto to a file.\n",
      "        \n",
      "        The graph is written as a binary proto unless `as_text` is `True`.\n",
      "        \n",
      "        ```python\n",
      "        v = tf.Variable(0, name='my_variable')\n",
      "        sess = tf.Session()\n",
      "        tf.train.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')\n",
      "        ```\n",
      "        \n",
      "        Args:\n",
      "          graph_def: A `GraphDef` protocol buffer.\n",
      "          logdir: Directory where to write the graph. This can refer to remote\n",
      "            filesystems, such as Google Cloud Storage (GCS).\n",
      "          name: Filename for the graph.\n",
      "          as_text: If `True`, writes the graph as an ASCII proto.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['RMSPropOptimizer', 'ClusterSpec', 'shuffle_batch_join', 's...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
