{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epochs', 5, 'Number of epochs to run trainer.')\n",
    "flags.DEFINE_integer('batch_size', 4096, 'Batch size.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = tf.train.match_filenames_once(\"data_v2/*\")\n",
    "train_files = data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(serialized_example):\n",
    "    \"\"\"\n",
    "    Extracts a `dict` of named features from the serialized `tf.train.Example`\n",
    "    \"\"\"\n",
    "    return tf.parse_single_example(\n",
    "        serialized=serialized_example,\n",
    "        features={\n",
    "            'input_dense_dimensions': tf.FixedLenFeature([2], dtype=tf.int64),\n",
    "            'sparse_index_dimensions': tf.FixedLenFeature([2], dtype=tf.int64),\n",
    "            'input': tf.FixedLenFeature([80], dtype=tf.int64),\n",
    "            'label': tf.FixedLenFeature([1], dtype=tf.int64),\n",
    "            'label_length': tf.FixedLenFeature([1], dtype=tf.int64),\n",
    "        }\n",
    "    )\n",
    "\n",
    "def deserialize_example(serialized_example):\n",
    "    \"\"\"\n",
    "    Converts a serialized `tf.train.Example` to FP32 Tensors\n",
    "    \"\"\"\n",
    "    features = extract_features(serialized_example)\n",
    "    shape_sparse = tf.cast(features['sparse_index_dimensions'], tf.int32)\n",
    "    indices = tf.reshape(tf.cast(features['input'], tf.int32), shape_sparse)\n",
    "    values = tf.ones([shape_sparse[0]])\n",
    "    shape_dense = tf.cast(features['input_dense_dimensions'], tf.int32)\n",
    "    input = tf.sparse_to_dense(indices, (20, 111), values)\n",
    "    label = tf.one_hot(features['label'][0], 99, on_value=1., off_value=0., dtype=tf.float32)\n",
    "    return input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    \"\"\"\n",
    "    Read and Deserialize a single `tf.train.Example` from a TFRecord file.\n",
    "    \"\"\"\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    return deserialize_example(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size=None, num_epochs=None, train=True):\n",
    "    \"\"\"\n",
    "    Read in shuffled `inputs` and `labels` from either train or val files.\n",
    "    \n",
    "    Returns:\n",
    "        `inputs` : [batch_size, 20, 111]\n",
    "        `labels` : [batch_size, 99]\n",
    "    \"\"\"\n",
    "    batch_size = batch_size or 128\n",
    "    num_epochs = num_epochs or 1\n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer(train_files, num_epochs=num_epochs)\n",
    "        input, label = read_and_decode(filename_queue)\n",
    "        inputs, labels = tf.train.shuffle_batch(\n",
    "            [input, label],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=20,\n",
    "            capacity=4*batch_size,\n",
    "            min_after_dequeue=batch_size\n",
    "        )\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the ORNL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.objectives import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_size = 50\n",
    "seq_length = 20\n",
    "num_vocab = 99\n",
    "num_classes = 12\n",
    "\n",
    "batch_size=1024\n",
    "num_epochs=10\n",
    "\n",
    "with tf.name_scope(\"nn\"):\n",
    "    model = Sequential()\n",
    "    model.add( LSTM(rnn_size, input_shape=(seq_length, num_vocab + num_classes)) )\n",
    "    model.add( Dense(num_vocab) )\n",
    "    model.add( Activation('softmax') )\n",
    "\n",
    "\n",
    "inputs, labels = get_batch(batch_size=batch_size, num_epochs=num_epochs)    \n",
    "logits = model(inputs)\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, logits))\n",
    "    \n",
    "train_step = tf.train.RMSPropOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.59196 1.00778007507\n",
      "50 4.60524 0.367364168167\n",
      "100 4.53334 0.353302001953\n",
      "150 3.92899 0.354943037033\n",
      "200 3.63322 0.373548984528\n",
      "250 3.5484 0.346050977707\n",
      "300 3.45455 0.363458871841\n",
      "350 3.38735 0.357393026352\n",
      "400 3.31312 0.365509986877\n",
      "450 3.27416 0.356905937195\n",
      "500 3.34267 0.365943193436\n",
      "550 3.21976 0.365089893341\n",
      "600 3.19046 0.36742401123\n",
      "650 3.20465 0.34531211853\n",
      "700 3.1037 0.367758989334\n",
      "750 3.20113 0.368250131607\n",
      "800 3.12648 0.366876840591\n",
      "850 3.1324 0.349768161774\n",
      "900 3.04707 0.370870828629\n",
      "950 3.03718 0.34969496727\n",
      "1000 3.00625 0.374936819077\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    keras.backend.set_session(sess)\n",
    "    \n",
    "    init_op = tf.group(tf.initialize_all_variables(),\n",
    "                       tf.initialize_local_variables())\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss_value = sess.run([train_step, loss]) #, feed_dict={\n",
    "            #        inputs: batch[0].eval(),\n",
    "            #        labels: batch[1].eval()\n",
    "            #})\n",
    "        \n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Print an overview fairly often.\n",
    "            if step % 50 == 0:\n",
    "                print(step, loss_value, duration)\n",
    "    \n",
    "            step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (10, step))\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
